{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47d2e7c-4876-40b8-9bdf-56c51cf6fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training progress bar\n",
    "# !pip install -q qqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3aa039-b0e1-49c1-81be-1dda5f7adecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 24 15:27:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1A:00.0 Off |                  N/A |\n",
      "| 27%   31C    P8    19W / 250W |   1224MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8    15W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:3E:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8     2W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8    15W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8     1W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     33132      C   python3                          1221MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db80dc04-f335-4848-bcbb-47aeab77cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate  https://github.com/MachineLearningHW/HW8_Dataset/releases/download/v1.0.0/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03bf174b-285e-443a-a482-7b2c8158ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch.optim import Adam, AdamW\n",
    "from qqdm import qqdm, format_str\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a1212d-aa0d-4ff8-b78c-68d6182c7cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 64, 64, 3)\n",
      "(19636, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = np.load('/home/jovyan/homework/dataset/hw8/data/trainingset.npy', allow_pickle=True)\n",
    "test = np.load('/home/jovyan/homework/dataset/hw8/data/testingset.npy', allow_pickle=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a628dffa-3d25-49a8-9188-b8e6882adffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(48763)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56726eee-2335-4f5c-847b-6c6d3fc23e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcn_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fcn_autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64 * 64 * 3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(64, 12), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12, 3)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 64 * 64 * 3), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class conv_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),         \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),        \n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(24, 48, 4, stride=2, padding=1),         \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "\t\t\t      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\t\t\t      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),    \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_out_1 = nn.Sequential(\n",
    "            nn.Conv2d(24, 48, 4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.enc_out_2 = nn.Sequential(\n",
    "            nn.Conv2d(24, 48, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "\t\t\t      nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "\t\t\t      nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1), \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        return self.enc_out_1(h1), self.enc_out_2(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def loss_vae(recon_x, x, mu, logvar, criterion):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    mse = criterion(recon_x, x)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    return mse + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934506af-5edf-4b51-ab26-5229c4316cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "        if tensors.shape[-1] == 3:\n",
    "            self.tensors = tensors.permute(0, 3, 1, 2)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "          transforms.Lambda(lambda x: x.to(torch.float32)),\n",
    "          transforms.Lambda(lambda x: 2. * x/255. - 1.),\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            # mapping images to [-1.0, 1.0]\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06576f7-a186-4802-bb98-9056feaf2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Build training dataloader\n",
    "x = torch.from_numpy(train)\n",
    "train_dataset = CustomTensorDataset(x)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model_type = 'vae'   # selecting a model type from {'cnn', 'fcn', 'vae', 'resnet'}\n",
    "model_classes = {'fcn': fcn_autoencoder(), 'cnn': conv_autoencoder(), 'vae': VAE()}\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "i=4\n",
    "device =torch.device(f'cuda:{i}')\n",
    "\n",
    "\n",
    "# model = model_classes[model_type].cuda()\n",
    "model = model_classes[model_type].to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94bbbe5-367c-4667-9dd0-c997ea331c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
      " \u001b[99m0/\u001b[93m50\u001b[0m\u001b[0m   \u001b[99m        -        \u001b[0m  \u001b[99m   -    \u001b[0m                                             \n",
      "\u001b[1mDescription\u001b[0m   0.0% |                                                           |\u001b[K\u001b[F\u001b[K\u001b[F \u001b[1mIters\u001b[0m    \u001b[1mElapsed Time\u001b[0m      \u001b[1mSpeed\u001b[0m                                               \n",
      " \u001b[99m0/\u001b[93m50\u001b[0m\u001b[0m   \u001b[99m        -        \u001b[0m  \u001b[99m   -    \u001b[0m                                             \n",
      "\u001b[1mDescription\u001b[0m   0.0% |                                                           |"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:4!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ===================forward=====================\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_vae(output[\u001b[38;5;241m0\u001b[39m], img, output[\u001b[38;5;241m1\u001b[39m], output[\u001b[38;5;241m2\u001b[39m], criterion)\n",
      "File \u001b[0;32m~/anaconda3/envs/knowledgegraphcommon-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    100\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m--> 101\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreparametrize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z), mu, logvar\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mVAE.reparametrize\u001b[0;34m(self, mu, logvar)\u001b[0m\n\u001b[1;32m     92\u001b[0m     eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(std\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mnormal_()\n\u001b[1;32m     93\u001b[0m eps \u001b[38;5;241m=\u001b[39m Variable(eps)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(mu)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:4!"
     ]
    }
   ],
   "source": [
    "\n",
    "best_loss = np.inf\n",
    "model.train()\n",
    "\n",
    "qqdm_train = qqdm(range(num_epochs), desc=format_str('bold', 'Description'))\n",
    "for epoch in qqdm_train:\n",
    "    tot_loss = list()\n",
    "    for data in train_dataloader:\n",
    "\n",
    "        # ===================loading=====================\n",
    "        img = data.float().to(device)\n",
    "        if model_type in ['fcn']:\n",
    "            img = img.view(img.shape[0], -1)\n",
    "\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        if model_type in ['vae']:\n",
    "            loss = loss_vae(output[0], img, output[1], output[2], criterion)\n",
    "        else:\n",
    "            loss = criterion(output, img)\n",
    "\n",
    "        tot_loss.append(loss.item())\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================save_best====================\n",
    "    mean_loss = np.mean(tot_loss)\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = mean_loss\n",
    "        torch.save(model, 'best_model_{}.pt'.format(model_type))\n",
    "    # ===================log========================\n",
    "    qqdm_train.set_infos({\n",
    "        'epoch': f'{epoch + 1:.0f}/{num_epochs:.0f}',\n",
    "        'loss': f'{mean_loss:.4f}',\n",
    "    })\n",
    "    # ===================save_last========================\n",
    "    torch.save(model, 'last_model_{}.pt'.format(model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2711f99-846c-427c-993b-ccff2240e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 200\n",
    "\n",
    "# build testing dataloader\n",
    "data = torch.tensor(test, dtype=torch.float32)\n",
    "test_dataset = CustomTensorDataset(data)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=eval_batch_size, num_workers=1)\n",
    "eval_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "# load trained model\n",
    "checkpoint_path = f'last_model_{model_type}.pt'\n",
    "model = torch.load(checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "# prediction file \n",
    "out_file = 'prediction.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27d517-b5a9-4cfd-8153-63bfad0adadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomality = list()\n",
    "with torch.no_grad():\n",
    "  for i, data in enumerate(test_dataloader):\n",
    "    img = data.float().to(device)\n",
    "    if model_type in ['fcn']:\n",
    "      img = img.view(img.shape[0], -1)\n",
    "    output = model(img)\n",
    "    if model_type in ['vae']:\n",
    "      output = output[0]\n",
    "    if model_type in ['fcn']:\n",
    "        loss = eval_loss(output, img).sum(-1)\n",
    "    else:\n",
    "        loss = eval_loss(output, img).sum([1, 2, 3])\n",
    "    anomality.append(loss)\n",
    "anomality = torch.cat(anomality, axis=0)\n",
    "anomality = torch.sqrt(anomality).reshape(len(test), 1).cpu().numpy()\n",
    "\n",
    "df = pd.DataFrame(anomality, columns=['score'])\n",
    "df.to_csv(out_file, index_label = 'ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledgegraphcommon-py3.8",
   "language": "python",
   "name": "knowledgegraphcommon-py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
