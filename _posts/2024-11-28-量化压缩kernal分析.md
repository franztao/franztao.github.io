子任务：

•1. 采用常见深度学习框架实现一个经典模型

•框架可以选：tensorflow/pytorch/caffe

•经典模型可以从前文算法模型中选择一个

•最好是从0开始搭建，不要直接将网上模型下载下来，理解深度学习模型中的基本元素，包括节点，边，层，图等。

•2. 基于训练集设计预处理、loss并训练到收敛

•设计图像预处理逻辑，理解预处理对训练收敛的意义

•自行设计loss，理解loss含义及在训练任务中的作用

•采用mnist训练集将模型训练到收敛，且精度达到95%以上，越高越好



子任务：

•3. 导出fp32推理模型，将fp32推理模型转换为onnx fp32模型

•采用开源的模型转换工具，将训练框架的模型转换为onnx模型

•4. 编写onnxruntime测试程序

•采用cmake搭建测试工程的编译方法

•采用git管理测试代码

•5. 基于onnxruntime cuda EP，测试模型延迟（latency）和吞吐（throughput）

•采用NVIDIA docker搭建cuda测试环境

•测试fp32模型在NV GPU上的延迟和吞吐

•6. 通过onnxruntime量化得到int8 qdq模型，测试模型在cuda EP上的精度、延迟和吞吐

•与fp32模型比较精度、延迟和吞吐



子任务：

•7. 对比onnxruntime不同优化级别，模型结构区别及执行区别

•测试onnxruntime在不同优化级别下，经过图优化后的模型结构区别

•8. 采用TensorRT执行fp32模型，并量化到int8模型，跟onnxruntime对比精度、延迟和吞吐

•9. 通过nsight systems分析以上各种情况下模型结构区别和GPU层面的执行区别：kernel数量、kernel融合、kernel耗时、整体耗时等

