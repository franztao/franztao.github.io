---
layout:     post
title:      <chatGPT>学习笔记
subtitle:   2022年12月
date:       2022-12-10
author:     franztao
header-img: post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - GPT
    - Generative Pre-Training

---

李宏毅老师新鲜出炉的关于ChatGPT的解读视频，非常推荐：[Chat GPT (可能)是怎麼煉成的 - GPT 社會化的過程 - YouTube](https://www.youtube.com/watch?v=e0aKI2GGZNg&t=540s)

原文链接：[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)

## GPT Introduction

[李宏毅老师 GPT3 ppt](http://speech.ee.ntu.edu.tw/~tlkagk/courses/DLHLP20/GPT3%20(v6).pdf)

模型之大，感官认识如下图说明

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-21-08-21-image.png)

模型整体架构

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-21-13-19-image.png)

## chatGPT  Introduction

### lihongyi讲解主要内容

1. 学习文字接龙

    2.  人类老师引导文字接龙的方向

3. 模仿人类老师的喜好

4. 用增强学习向模拟老师学习

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-20-55-56-image.png)

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-20-56-19-image.png)

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-20-56-36-image.png)

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-20-57-10-image.png)

## OpenAI官网内容解读

OpenAI 开放了一个支持交互式对话的 ChatGPT 模型，可以遵循提示中的指令并提供详细的响应：回答问题、承认错误、拒绝不适当的请求等。以下是官方示例：

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-20-37-36-image.png)

团队使用了 RLHF（Reinforcement Learning from Human Feedback，来自人类反馈的强化学习）训练模型。

> - 步骤1：人工智能训练师分别扮演用户、AI助手两个角色进行对话，训练初始模型，并确定回答的基本策略。
> 
> - 步骤2：人工智能培训师与聊天机器人进行对话，收集比较数据（包括两个或多个按质量排名的模型响应），创建强化学习的奖励模型。
> 
> - 步骤3：使用近端策略优化（Proximal Policy Optimization）完成对上述模型的微调，并在此过程执行多次迭代。

![](C:\Users\franztao\AppData\Roaming\marktext\images\2022-12-10-20-37-51-image.png)





# 也聊一下ChatGPT

AINLP *2022-12-11 22:56* *发表于江苏*

以下文章来源于多头注意力 ，作者多头注意力

[

![](http://wx.qlogo.cn/mmhead/Q3auHgzwzM7gOP140M9eIVJJfYqU6hEib3qkx2ln1lOG7fGPlb0fchw/0)

**多头注意力**.

AI算命，朋克养生

](https://mp.weixin.qq.com/s/pYUa6Vo3_Ega8yjETBRNIQ#)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/nW2ZPfuYqSJuK8UUBxdZXj1c20hUg374YPgXibgDGytAy87YxvVk4WCRFWrdKJPshStrlPJp4vGEGUQodxt7ibOw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

本文不长，也适合没有技术背景的朋友阅读。

最近ChatGPT火了，而且火出圈了。好多不是做技术的朋友都开始关注甚至转发相关文章。从广为流传的一些例子看，ChatGPT确实做出了让大家眼前一亮的效果。前两年的Meena和dialogGPT虽然强，但都没有超过这个出圈的阈值。聊天机器人搞了这么些年，也终于有了一个让大家都比较认可的产品。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/fI41EfAgQvskFw33MNC4Qia4EJ7InpqIsiaFCAxPEG7LxKKBaa4J27NfMzZx5N1yfAjegFzicFAlWiaDDpia1OPsIkw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

ChatGPT的结果令人惊艳

## 小迷思

前几天几个小伙伴聊天，说起ChatGPT和OpenAI，纷纷感叹为什么国内没有这样的创新公司和突破性产品涌现。几个大厂的研究院，比如阿里达摩院、字节AI Lab虽然成果也很多，但跟DeepMind、OpenAI比起来差距还是很大。其实ChatGPT背后的东西并不是有多难，但为什么做出来的是他们？

今天在知乎上发现也有类似的问题，还挺火的。不少回答都从大环境的角度分析，有说我们还穷的，有说国内资源和人才不匹配的。这些固然对，但作为个体我们也可以从自己身上找找原因。前几天看到一个做AI架构的大佬在朋友圈感叹，18年就在某大厂实现了500块GPU并行训练transformer，但大家都不知道这东西能干嘛。所以有的时候并不全是资源不到位的问题。我不禁想起了马老师“因为相信，所以看见”的观点，我感觉就是差在这个境界上。从学校毕业五年多了，我感觉这也是自己目前比较大的一个问题，我们有把事情做好的能力，但却缺少真正相信且原意长期坚持的东西。

## ChatGPT背后的技术

还是聊回技术。ChatGPT还没有公开的论文，根据OpenAI的博客[1]，基本上使用的技术和他们在今年早些时候公布的InstructGPT[2]差不多。

> We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup.

![图片](https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvskFw33MNC4Qia4EJ7InpqIsibwfkbicdUPq9Mtic4CMWrudpvOa1WLKLsfExXnjMW1tdjLB6ISs3L0uA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

ChatGPT训练流程

上面是ChatGPT博客上的训练流程图，下面是早先InstructGPT论文里的训练流程图，嗯，可以说是一模一样，比较大的差别是基础语言模型从GPT3升级到了GPT3.5。

![图片](https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvskFw33MNC4Qia4EJ7InpqIs4ugmgNtkM0icd42wTtRxfYeqqVjKiahKhW919QvgLwODwNQGCXk4icH5g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

InstructGPT训练流程

InstructGPT的介绍有图例，更容易讲清楚ChatGPT是如何训练的。这个模型的训练分为3个步骤：

1. 从预训练语言模型出发，用标注者产生的数据fine tune一个根据提示（prompt）生成答案的模型，这一步称为SFT.

2. 用上一步训练的模型生成大量的答案，每一个prompt都生成多组，并让标注者对这些答案进行排序。用这样获得的数据训练一个奖励模型（Reward Model，RM）。这个模型会作为后续强化学习环节的世界模型。

3. 强化学习训练。这一步有点左右互搏的意思，用RM模型作为世界模型，SFT之后的生成模型做agent，进行训练，让生成模型尽可能地在RM模型那里拿到高分。这一步使用的算法也来自OpenAI，为2017年发布的PPO算法。

![图片](https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvskFw33MNC4Qia4EJ7InpqIsRpB2yd7pwmYGq8BW3GKklqibd5r2YVicKeQdKdnyicnOvRIcGxsiaytb5w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

强化学习一般流程，让Agent学会在Environment里获得更多的Reward。在InstructGPT训练过程中，每个action对应生成模型产生一个token

我对强化学习并不熟悉，只是稍微看了下PPO[3]的介绍，这个算法的目标函数可以用下面的公式来概括，这个目标函数包含了三个部分，第一部分是标准的强化学习目标，即在reward model那得高分；第二部分是PPO的创新点，即KL惩罚，目的是让强化学习的每一步都不要更新太多（新模型和老模型的KL散度要小）；第三部分是针对语言模型精调新加的，为了防止语言模型在精调的时候退化，他们在精调时把语言模型loss也加了进来。三个部分通过两个超参数β和γ进行调节。

![图片](https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvskFw33MNC4Qia4EJ7InpqIsibxkNWIE0GPBRPYJByNp8ha9OONskMDgCiaDdWjc25xsKuou5X6euQ7A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

PPO环节的目标函数

ChatGPT没有公布具体的训练数据情况，但从InstructGPT论文看，训练一个这样的模型并不需要特别多的数据，跟语言模型的训练数据比起来差了好几个数量级。从论文可以看出，用户在他们网站上调戏机器人的数据都有可能被用来做后续模型的训练，所以大家可以感谢自己为人工智能的发展做出的贡献。

> The SFT dataset contains about 13k training prompts (from the API and labeler-written), the RM dataset has 33k training prompts (from the API and labeler-written), and the PPO dataset has 31k training prompts (only from the API).

再往前追溯的话，这种RLHF技术出现在2020年OpenAI的论文Learning to summarize from human feedback[4]。这篇论文的工作是使用RLHF技术来改进摘要生成模型。论文里也有一张训练流程图，跟上面两张几乎一样，可以说已经打下了后面两个工作的基础。也就是说这项工作并不是什么突然爆发的突破，而是一个持续了已经有三年的项目，如果追溯到PPO的话，那更是已经有5年了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/fI41EfAgQvskFw33MNC4Qia4EJ7InpqIsvHEWoeO62xciaPIjiaG2Zo1Fp5u9pJSRzTWibWVbpK7VpHfyyhM5lNEYg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

Learning to summarize from human feedback训练流程。已经跟后面两个模型很接近了。

## 我怎么看ChatGPT

不管在网上看到的效果多惊艳，都不能改变ChatGPT是个统计学习模型的事实。它的能力其实绝大部分来源于GPT-3.5这个强大的语言模型，而语言模型1730 亿权重刻画的本质上还是字词的共现规律。它并不理解你说的是什么，只是对于你打下的问题，他可以得到一个符合训练数据中的统计规律且能在reward model那里得到高分的答案。

网上有很多关于它会不会替代搜索引擎的讨论，我感觉这根本不是一个层面的东西。搜索引擎虽然有排序，但本质上还是你来在信息间做选择。但在对话里，你只能选择信或者不信。而且把知识固化在参数里也意味着不好更新，ChatGPT虽然见多识广，但记忆只停留在2021年。

网上还有不少看热闹不嫌事大的文章鼓吹这个技术的诞生会让多少人的饭碗消失。这个担忧，我倒觉得不无道理。ChatGPT给出的是符合概率分布的结果，并不是出众的结果，和它类似的东西其实还有很多。比如我常常使用的作图工具“可画”，可以用它快速得到一些还看得过去的素材，但这些素材和艺术肯定不沾边。很多白领工作会因为这些工具的诞生蓝领化，然后无人化。作为人，我们只能尽量让自己有输出“分布外”结果的能力。就像不管机床多精密，机器人多牛逼，大师的雕塑还是可以价值连城。

最后，推荐一个OpenAI CEO的访谈视频

### 参考资料

[1]

OpenAI关于ChatGPT的博客: *https://openai.com/blog/chatgpt/*

[2]

Training language models to follow instructions with human feedback: *https://arxiv.org/abs/2203.02155v1*

[3]

Proximal Policy Optimization: *https://openai.com/blog/openai-baselines-ppo/*

[4]

Learning to summarize from human feedback: *https://arxiv.org/abs/2009.01325v3*

![](http://mmbiz.qpic.cn/mmbiz_png/nW2ZPfuYqSKAnqcViaIWJ40GKChb8uoLoichr44nhO2OSPBzMxhdosvv3Ljc695woogSyoIMlsiaIRuk8ur4jpHZg/0?wx_fmt=png)

**AINLP**

一个有趣有AI的自然语言处理公众号：关注AI、NLP、机器学习、推荐系统、计算广告等相关技术。公众号可直接对话双语聊天机器人，尝试自动对联、作诗机、藏头诗生成器，调戏夸夸机器人、彩虹屁生成器，使用中英翻译，查询相似词，测试NLP相关工具包。

353篇原创内容

公众号

**进技术交流群请添加AINLP小助手微信（id: ainlp2)**  

**请备注具体方向+所用到的相关技术点**

**关于AINLP**

AINLP 是一个有趣有AI的自然语言处理社区，专注于 AI、NLP、机器学习、深度学习、推荐算法等相关技术的分享，主题包括文本摘要、智能问答、聊天机器人、机器翻译、自动生成、知识图谱、预训练模型、推荐系统、计算广告、招聘信息、求职经验分享等，欢迎关注！加技术交流群请添加AINLP小助手微信(id：ainlp2)，备注工作/研究方向+加群目的。

**阅读至此了，分享、点赞、在看三选一吧🙏**

喜欢此内容的人还喜欢

NLP模式高效匹配技术总结：模式匹配常见落地场景、AC自动机原理及高效开源工具总结

老刘说NLP

不喜欢

不看的原因

确定

- 内容质量低
- 不看此公众号

![](http://mmbiz.qpic.cn/mmbiz_jpg/fUBU1yiaEmJjtt47wojxhE2JFzF4XiasfU10BFGAZImHXbge12k66DD8D9l8XicLPPhjvthullt03bgTsDuj5pOTQ/0?wx_fmt=jpeg)

图解 SQL 的执行顺序

Python开发者

不喜欢

不看的原因

确定

- 内容质量低
- 不看此公众号

![](https://mmbiz.qpic.cn/mmbiz_jpg/fhujzoQe7TpkibVZyXlT87iaxjTyvRGNwIq7KH0RE1Cx6LoqThxO8RicWeZaxcicicZJA5R32XhFUzhDZQLQjjDCPhQ/0?wx_fmt=jpeg)

算法性能分析，怎么判断我的算法会不会超时？

Coder梁

不喜欢

不看的原因

确定

- 内容质量低
- 不看此公众号

![](https://mmbiz.qpic.cn/mmbiz_jpg/6nNCNSoRubwFUyzPibqib7QAobgGjL7GXdBdjIiaeB9B717hfY2etYhjnHpCXupUCMORlqt9h2znibyNqcON6oa6qw/0?wx_fmt=jpeg)

![](https://mp.weixin.qq.com/mp/qrcode?scene=10000004&size=102&__biz=MjM5ODkzMzMwMQ==&mid=2650434873&idx=3&sn=ab7963aa626fbbbdff829e46cc0a4eb9&send_time=)

微信扫一扫  
关注该公众号