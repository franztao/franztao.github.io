---
layout:     post
title:      多模态RAG
subtitle:   2025年5月
date:       2025-05-23
author:     franztao
header-img: post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - multimodel
    - RAG
    - 文档智能

---





# 多模态RAG调研


# 建立多模态RAG技术能力，需要解决以下几点问题

## 如何有效地解析和索引多模态文档

MRAG系统需要对多模态文档进行解析和索引。这包括**提取文本内容(使用OCR或特定格式的解析技术从多模态文档中提取文本内容)、检测文档布局并将其分割成结构化元素（如标题、段落、图像、视频等）**。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/4jKqmVXRJQ8GOw19/img/d9fcaa91-dd01-4fc3-afb2-989082546f9c.png)

## 如何建立多模态index与进行多模态检索

方法分为三类：

(a) 单模态单stream检索，将所有模态统一到单一（文本）的综合模态中；

(b) 跨模态单stream检索，将所有模态嵌入到一个共享的向量空间中；

(c) 单模态多stream检索，为每种模态分别维护独立的数据库。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/yBRq1ZvwwpzbxOdv/img/3e2029c1-aa7a-401c-b28e-d401708a93c8.png)

## 2.3 如何在生成过程中整合多模态数据

多模态大型语言模型（MLLM）是基于Transformer的LLM，它们经过多模态数据（包括文本、图像、表格、音频和视频）的预训练和微调，以分析和理解各种数据格式 ,代表性模型包括GPT-4o。

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/yBRq1ZvwwpzbxOdv/img/715e6abf-4ed1-4fb1-a17a-ba524e7da644.png)

## 2.4 如何评估和改进MRAG系统的性能

**评估MRAG系统的质量分为三个方面**

**检索评估：**

*   **命中率：** 查询中正确文档出现在前N个结果中的百分比 。   
    
*   **平均倒数排名（MRR）：** 量化最有用的文档是否排名更高 。   
    
*   **相关性分数（RS）：** 评估检索到的条目与查询的相关性（适用于多模态数据），例如评估检索到的图像或文本与用户意图的对齐程度 。   
    
*   **上下文召回率：** 衡量检索到的上下文与真实答案的匹配程度 。   
    
*   **上下文精度：** 评估最相关的上下文项是否排名高于不相关的项 。
    

**生成评估：**

*   **正确性分数（CS）：** 评估生成响应相对于原始上下文的准确性 。   
    
*   **多模态答案相关性：** 衡量多模态RAG管道生成器输出与提供输入的关联程度 。   
    
*   **多模态忠实度：** 评估生成输出是否与检索上下文的内容事实性对齐 。   
    
*   **图像连贯性：** 评估图像与伴随文本的对齐程度 。   
    
*   **图像帮助性：** 评估图像对用户理解的有效贡献 。   
    
*   **图像引用：** 衡量文本引用或解释图像的准确性 。   
    
*   传统的NLP指标（BLEU、ROUGE）可以使用，但单独使用往往不足 。事实一致性检查（例如使用BERTScore）至关重要 。 
    

**端到端性能：**

*   **延迟：** 查询的端到端处理时间 。   
    
*   **吞吐量：** 单位时间内处理的查询数量 。   
    
*   **资源利用率：** CPU、GPU和内存使用情况 。   
    
*   **错误率、用户满意度和任务成功率（A/B测试）** 。   
    
*   **人工评估**对于判断正确性、清晰度、完整性以及识别幻觉至关重要 。
    

**评估MRAG系统的基准与数据集**

**MRAG-Bench**，以视觉为中心的LVLM基准

**M2RAG**，用于评估MLLM利用多模态检索文档能力的基准

**TEMPRAGEVAL**，针对时间敏感型问答

# 多模态RAG的三个版本

## MRAG1.0

MRAG1.0 被称为"伪MRAG”，通过将多模态数据转换为统一的文本表示，利用现有的文本检索和生成机制实现了从RAG到MRAG的无缝衔接

### 框架图

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/yBRq1ZvwwpzbxOdv/img/9ecdd442-cc3c-45ca-b824-0042bdaee1b7.png)

## MRAG2.0

MRAG2.0进入了"真正的多模态”时代，**支持用户具有多模态输入的查询，并保留知识库中的原始多模态数据**。通过利用MLLMs的能力，生成模块可以直接处理多模态数据，从而最小化数据转换期间的信息损

失。

### 框架图

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/yBRq1ZvwwpzbxOdv/img/e395c07c-6a39-4f78-9fa3-1484b387b5d3.png)

## MRAG3.0

MRAG3.0代表了一个重要的进化，引入了结构和功能创新，增强了多个维度的能力。新范式的特点包括增强的文档解析、端到端多模态性和场景扩展。在输出阶段，**多模态检索增强组合模块通过将纯文本转换为多模态格式来增强答案生成**，从而丰富信息传递

### 框架图

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/yBRq1ZvwwpzbxOdv/img/6c00ba3d-7a2a-4c10-bdbe-156e9aa2dabc.png)

## 在现有 Demo 中（ dify） 需要补足的能力

| **RAG流程**   | **功能/模型**                              | **哪个版本需要(1指MRAG1.0版本)** |
| ------------- | ------------------------------------------ | -------------------------------- |
| 文档智能      | 多格式文档解码工具                         | 1,2,3                            |
|               | CV,目标检测，版面分析模型                  | 1,2,3                            |
|               | CV,文本检测与识别ocr模型                   | 1,2,3                            |
|               | CV,表格识别模型                            | 1                                |
|               | MLLM模型，图像理解方向                     | ### 2,3                          |
|               | image/table caption知识库，文本格式        | ### 1,2,3                        |
|               | 多模态知识库，图片格式                     | 2,3                              |
|               | 文件单页截图知识库                         | 3                                |
| 检索          | 多模态embeding模型                         | 2,3                              |
|               | 多模态向量库                               | 2,3                              |
|               | 多模态rerank模型                           | 2,3                              |
|               | 文件单页截图检索模型                       | 3                                |
|               | 文件单页截图向量库                         | 3                                |
|               | 文本web搜索功能                            | 3                                |
|               | 多模态web搜索功能                          | 3                                |
| query与上下文 | 多轮对话文本中带有图片信息                 | 2,3                              |
|               | 多模态query搜索规划模型                    | 3                                |
|               | query是否触发web搜索判断模型               | 3                                |
| 生成          | 多模态prompt设计                           | 2,3                              |
|               | MLLM模型，文本生成方向                     | 2,3                              |
|               | 多模态augment输出,文生图模型，版面编排模型 | 3                                |

## 三个版本的区别

| **特性**     | **MRAG1.0 (伪MRAG)**                                         | **MRAG2.0 (真多模态)**                                       | **MRAG3.0 (结构与功能创新)**                                 |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据处理方式 | 将多模态数据转换为统一的文本表示，再进行检索和生成           | 保留原始多模态数据，直接利用MLLMs处理多模态数据              | 在输入、检索和输出阶段均支持端到端多模态处理，引入了多模态输出能力 |
| 检索能力     | 基于文本向量的检索技术，无法充分利用跨模态信息的优势         | 支持跨模态检索，能够结合文本和多模态数据进行更精确的检索     | 引入动态检索规划模块，优化检索效率并减少无关信息的干扰。     |
| 生成能力     | 生成纯文本答案，容易因数据转换导致信息丢失                   | 能够生成结合多模态数据的答案，显著减少信息损失               | 支持多模态输出（如图文结合），进一步丰富生成内容的表现形式。 |
| 主要局限性   | *   数据转换过程复杂      *    易丢失细粒度跨模式信息      *   检索准确率受限 | *   业界多模态检索baseline能力不及文件检索      *   需要高效组织多样化数据格式 | 系统复杂度极高，对计算资源要求更高                           |
| 应用场景     | 主要适用于以文本为主、多模态为辅的应用场景                   | 更适合需要结合多模态数据进行理解和生成的任务，如视觉问答(VQA) | 覆盖范围更广包括检索增强、视觉问答(VQA)、图像描述等多种场景实现理解和生成能力的统一 |

# 参考资料

[https://arxiv.org/pdf/2504.08748](https://arxiv.org/pdf/2504.08748)

[https://gemini.google.com/app/4569e251136cfbd9?hl=zh](https://gemini.google.com/app/4569e251136cfbd9?hl=zh)

[https://mp.weixin.qq.com/s/kA53TFmcRcrXq6tenVKapg](https://mp.weixin.qq.com/s/kA53TFmcRcrXq6tenVKapg)

[https://mp.weixin.qq.com/s/MEgilART1t9KNEi82BScGQ](https://mp.weixin.qq.com/s/MEgilART1t9KNEi82BScGQ)

[https://mp.weixin.qq.com/s/l1NcfmuQ9CZKB0BIFu9m5g](https://mp.weixin.qq.com/s/l1NcfmuQ9CZKB0BIFu9m5g)

[https://mp.weixin.qq.com/s/Nn1GFGUniEPtOpeLa1\_I5Q](https://mp.weixin.qq.com/s/Nn1GFGUniEPtOpeLa1_I5Q)