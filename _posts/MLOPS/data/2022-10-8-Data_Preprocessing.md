---
layout:     post
title:      æ•°æ®é¢„å¤„ç†
subtitle:   2022å¹´10æœˆ
date:       2022-10-10
author:     franztao
header-img: post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Data Preprocessing

---



# æ•°æ®é¢„å¤„ç†

é€šè¿‡å‡†å¤‡å’Œè½¬æ¢å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ç”¨äºè®­ç»ƒã€‚



## Intuition

æ•°æ®é¢„å¤„ç†å¯ä»¥åˆ†ä¸ºä¸¤ç±»è¿‡ç¨‹ï¼š*å‡†å¤‡*å’Œ*è½¬æ¢*ã€‚å°†æ¢ç´¢å¸¸è§çš„é¢„å¤„ç†æŠ€æœ¯ï¼Œç„¶åé’ˆå¯¹ç‰¹å®šåº”ç”¨é€æ­¥å®Œæˆç›¸å…³è¿‡ç¨‹ã€‚

> è­¦å‘Š
> 
> æŸäº›é¢„å¤„ç†æ­¥éª¤`global`ï¼ˆä¸ä¾èµ–äºæ•°æ®é›†ï¼Œä¾‹å¦‚å°å†™æ–‡æœ¬ã€åˆ é™¤åœç”¨è¯ç­‰ï¼‰å’Œå…¶ä»–æ­¥éª¤`local`ï¼ˆç»“æ„ä»…ä»è®­ç»ƒæ‹†åˆ†ä¸­å­¦ä¹ ï¼Œä¾‹å¦‚è¯æ±‡ã€æ ‡å‡†åŒ–ç­‰ï¼‰ã€‚å¯¹äºæœ¬åœ°çš„ã€ä¾èµ–äºæ•°æ®é›†çš„é¢„å¤„ç†æ­¥éª¤ï¼Œè¦ç¡®ä¿åœ¨é¢„å¤„ç†ä¹‹å‰å…ˆ[æ‹†åˆ†](https://madewithml.com/courses/mlops/splitting/)æ•°æ®ä»¥é¿å…æ•°æ®æ³„æ¼ã€‚

## å‡†å¤‡ä¸­

å‡†å¤‡æ•°æ®æ¶‰åŠç»„ç»‡å’Œæ¸…ç†æ•°æ®ã€‚

### åŠ å…¥

ä¸ç°æœ‰æ•°æ®è¡¨æ‰§è¡Œ SQL è¿æ¥ï¼Œå°†æ‚¨éœ€è¦çš„æ‰€æœ‰ç›¸å…³æ•°æ®ç»„ç»‡åˆ°ä¸€ä¸ªè§†å›¾ä¸­ã€‚è¿™ä½¿å¾—ä½¿ç”¨æ•°æ®é›†å˜å¾—æ›´åŠ å®¹æ˜“ã€‚

```
SELECT * FROM A
INNER JOIN B on A.id == B.id

```

> è­¦å‘Š
> 
> éœ€è¦å°å¿ƒæ‰§è¡Œæ—¶é—´ç‚¹æœ‰æ•ˆè¿æ¥ä»¥é¿å…æ•°æ®æ³„æ¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¡¨ B å¯èƒ½å…·æœ‰è¡¨ A ä¸­çš„å¯¹è±¡çš„ç‰¹å¾ï¼Œè€Œè¿™äº›ç‰¹å¾åœ¨å½“æ—¶éœ€è¦è¿›è¡Œæ¨ç†æ—¶ä¸å¯ç”¨ã€‚

### ç¼ºå¤±å€¼

é¦–å…ˆï¼Œå¿…é¡»ç¡®å®šå…·æœ‰ç¼ºå¤±å€¼çš„è¡Œï¼Œä¸€æ—¦ç¡®å®šï¼Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥å¤„ç†å®ƒä»¬ã€‚

### å¼‚å¸¸å€¼ï¼ˆå¼‚å¸¸ï¼‰

- å…³äºä»€ä¹ˆæ˜¯â€œæ­£å¸¸â€é¢„æœŸå€¼çš„å·¥è‰ºå‡è®¾
  
  ```
  # Ex. Feature value must be within 2 standard deviations
  df[np.abs(df.A - df.A.mean()) <= (2 * df.A.std())]
  
  ```

- æ³¨æ„ä¸è¦åˆ é™¤é‡è¦çš„å¼‚å¸¸å€¼ï¼ˆä¾‹å¦‚æ¬ºè¯ˆï¼‰

- å½“åº”ç”¨è½¬æ¢ï¼ˆä¾‹å¦‚å¹‚å¾‹ï¼‰æ—¶ï¼Œå€¼å¯èƒ½ä¸æ˜¯å¼‚å¸¸å€¼

- å¼‚å¸¸å¯ä»¥æ˜¯å…¨å±€çš„ï¼ˆç‚¹ï¼‰ã€ä¸Šä¸‹æ–‡çš„ï¼ˆæœ‰æ¡ä»¶çš„ï¼‰æˆ–é›†ä½“çš„ï¼ˆä¸ªä½“ç‚¹ä¸å¼‚å¸¸ï¼Œé›†ä½“æ˜¯å¼‚å¸¸å€¼ï¼‰

### ç‰¹å¾å·¥ç¨‹

ç‰¹å¾å·¥ç¨‹æ¶‰åŠä»¥ç‹¬ç‰¹çš„æ–¹å¼ç»„åˆç‰¹å¾ä»¥æå–ä¿¡å·ã€‚

```
# Input
df.C = df.A + df.B

```



> tips
> 
> ç‰¹å¾å·¥ç¨‹å¯ä»¥ä¸é¢†åŸŸä¸“å®¶åˆä½œå®Œæˆï¼Œé¢†åŸŸä¸“å®¶å¯ä»¥æŒ‡å¯¼è®¾è®¡å’Œä½¿ç”¨å“ªäº›ç‰¹å¾ã€‚

### cleaning

æ¸…ç†æ•°æ®æ¶‰åŠåº”ç”¨çº¦æŸï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“ä»æ•°æ®ä¸­æå–ä¿¡å·ã€‚

- ä½¿ç”¨é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œ EDA

- é€šè¿‡è¿‡æ»¤å™¨åº”ç”¨çº¦æŸ

- ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´æ€§

- åˆ é™¤å…·æœ‰ç‰¹å®šåˆ—å€¼æˆ–ç©ºåˆ—å€¼çš„æ•°æ®ç‚¹

- å›¾åƒï¼ˆè£å‰ªã€è°ƒæ•´å¤§å°ã€å‰ªè¾‘ç­‰ï¼‰
  
  ```
  # Resize
  import cv2
  dims = (height, width)
  resized_img = cv2.resize(src=img, dsize=dims, interpolation=cv2.INTER_LINEAR)
  
  ```
  
  

- æ–‡æœ¬ï¼ˆä¸‹éƒ¨ã€è¯å¹²ã€è¯å½¢è¿˜åŸã€æ­£åˆ™è¡¨è¾¾å¼ç­‰ï¼‰
  
  ```
  # Lower case the text
  text = text.lower()
  
  ```
  
  

## è½¬æ¢

è½¬æ¢æ•°æ®æ¶‰åŠç‰¹å¾ç¼–ç å’Œå·¥ç¨‹ã€‚

### ç¼©æ”¾

- è¾“å…¥è§„æ¨¡å½±å“è¿‡ç¨‹çš„æ¨¡å‹éœ€è¦

- ä»è®­ç»ƒæ‹†åˆ†ä¸­å­¦ä¹ æ„é€ å¹¶åº”ç”¨äºå…¶ä»–æ‹†åˆ†ï¼ˆæœ¬åœ°ï¼‰

- ä¸è¦ç›²ç›®åœ°ç¼©æ”¾ç‰¹å¾ï¼ˆä¾‹å¦‚åˆ†ç±»ç‰¹å¾ï¼‰

- [æ ‡å‡†åŒ–](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)ï¼šå°†å€¼é‡æ–°è°ƒæ•´ä¸ºå‡å€¼ä¸º 0ï¼Œæ ‡å‡†ä¸º 1
  
  ```
  # Standardization
  import numpy as np
  x = np.random.random(4) # values between 0 and 1
  print ("x:\n", x)
  print (f"mean: {np.mean(x):.2f}, std: {np.std(x):.2f}")
  x_standardized = (x - np.mean(x)) / np.std(x)
  print ("x_standardized:\n", x_standardized)
  print (f"mean: {np.mean(x_standardized):.2f}, std: {np.std(x_standardized):.2f}")
  
  ```
  
  x: [0.36769939 0.82302265 0.9891467  0.56200803]
  mean: 0.69, std: 0.24
  x_standardized: [-1.33285946  0.57695671  1.27375049 -0.51784775]
  mean: 0.00, std: 1.00

- [min-max](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale)ï¼šåœ¨æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´é‡æ–°è°ƒæ•´å€¼
  
  ```
  # Min-max
  import numpy as np
  x = np.random.random(4) # values between 0 and 1
  print ("x:", x)
  print (f"min: {x.min():.2f}, max: {x.max():.2f}")
  x_scaled = (x - x.min()) / (x.max() - x.min())
  print ("x_scaled:", x_scaled)
  print (f"min: {x_scaled.min():.2f}, max: {x_scaled.max():.2f}")
  
  ```
  
  
  
  x: [0.20195674 0.99108855 0.73005081 0.02540603]
  min: 0.03, max: 0.99
  x_scaled: [0.18282479 1.         0.72968575 0.        ]
  min: 0.00, max: 1.00

- [åˆ†ç®±](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)ï¼šä½¿ç”¨åˆ†ç®±å°†è¿ç»­ç‰¹å¾è½¬æ¢ä¸ºåˆ†ç±»ç‰¹å¾
  
  ```
  # Binning
  import numpy as np
  x = np.random.random(4) # values between 0 and 1
  print ("x:", x)
  bins = np.linspace(0, 1, 5) # bins between 0 and 1
  print ("bins:", bins)
  binned = np.digitize(x, bins)
  print ("binned:", binned)
  
  ```
  
  x: [0.54906364 0.1051404  0.2737904  0.2926313 ]
  bins: [0.   0.25 0.5  0.75 1.  ]
  binned: [3 1 2 2]

- è¿˜æœ‰[æ›´å¤š](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)ï¼

### ç¼–ç 

- å…è®¸æœ‰æ•ˆåœ°è¡¨ç¤ºæ•°æ®ï¼ˆä¿æŒä¿¡å·ï¼‰å’Œæœ‰æ•ˆåœ°ï¼ˆå­¦ä¹ æ¨¡å¼ï¼Œä¾‹å¦‚ one-hot ä¸åµŒå…¥ï¼‰

- [label](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder)ï¼šåˆ†ç±»å€¼çš„å”¯ä¸€ç´¢å¼•
  
  ```
  # Label encoding
  label_encoder.class_to_index = {
  "attention": 0,
  "autoencoders": 1,
  "convolutional-neural-networks": 2,
  "data-augmentation": 3,
  ... }
  label_encoder.transform(["attention", "data-augmentation"])
  
  ```
  
  
  
  array([2, 2, 1])

- [one-hot](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)ï¼šè¡¨ç¤ºä¸ºäºŒè¿›åˆ¶å‘é‡
  
  ```
  # One-hot encoding
  one_hot_encoder.transform(["attention", "data-augmentation"])
  
  ```
  
  
  
  array([1, 0, 0, 1, 0, ..., 0])

- [åµŒå…¥](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)ï¼šèƒ½å¤Ÿè¡¨ç¤ºä¸Šä¸‹æ–‡çš„å¯†é›†è¡¨ç¤º
  
  ```
  # Embeddings
  self.embeddings = nn.Embedding(
      embedding_dim=embedding_dim, num_embeddings=vocab_size)
  x_in = self.embeddings(x_in)
  print (x_in.shape)
  
  ```
  
  
  
  (len(X), embedding_dim)

- è¿˜æœ‰[æ›´å¤š](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)ï¼

### Extraction

- ä»ç°æœ‰ç‰¹å¾ä¸­æå–ä¿¡å·

- ç»“åˆç°æœ‰åŠŸèƒ½

- [è¿ç§»å­¦ä¹ ](https://en.wikipedia.org/wiki/Transfer_learning)ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºç‰¹å¾æå–å™¨å¹¶å¯¹å…¶ç»“æœè¿›è¡Œå¾®è°ƒ

- [è‡ªåŠ¨](https://en.wikipedia.org/wiki/Autoencoder)ç¼–ç å™¨ï¼šå­¦ä¹ ç¼–ç å‹ç¼©çŸ¥è¯†è¡¨ç¤ºçš„è¾“å…¥

- [ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)ï¼šåœ¨è¾ƒä½ç»´ç©ºé—´ä¸­å¯¹é¡¹ç›®æ•°æ®è¿›è¡Œçº¿æ€§é™ç»´ã€‚
  
  ```
  # PCA
  import numpy as np
  from sklearn.decomposition import PCA
  X = np.array([[-1, -1, 3], [-2, -1, 2], [-3, -2, 1]])
  pca = PCA(n_components=2)
  pca.fit(X)
  print (pca.transform(X))
  print (pca.explained_variance_ratio_)
  print (pca.singular_values_)
  
  ```
  
  
  
  [[-1.44245791 -0.1744313]
   [-0.1148688 0.31291575]
   [ 1.55732672 -0.13848446]]
  [0.96838847 0.03161153]
  [2.12582835 0.38408396]

- [counts (ngram)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)ï¼šæ–‡æœ¬çš„ç¨€ç–è¡¨ç¤ºä½œä¸ºæ ‡è®°è®¡æ•°çŸ©é˜µâ€”â€”å¦‚æœç‰¹å¾å€¼æœ‰å¾ˆå¤šæœ‰æ„ä¹‰çš„ã€å¯åˆ†ç¦»çš„ä¿¡å·ï¼Œåˆ™å¾ˆæœ‰ç”¨ã€‚
  
  ```
  # Counts (ngram)
  from sklearn.feature_extraction.text import CountVectorizer
  y = [
      "acetyl acetone",
      "acetyl chloride",
      "chloride hydroxide",
  ]
  vectorizer = CountVectorizer()
  y = vectorizer.fit_transform(y)
  print (vectorizer.get_feature_names())
  print (y.toarray())
  # ğŸ’¡ Repeat above with char-level ngram vectorizer
  # vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 3)) # uni, bi and trigrams
  
  ```
  
  ['acetone', 'acetyl', 'chloride', 'hydroxide']
  [[1 1 0 0]
   [0 1 1 0]
   [0 0 1 1]]

- [similarity](https://github.com/dirty-cat/dirty_cat)ï¼šç±»ä¼¼äºè®¡æ•°å‘é‡åŒ–ï¼Œä½†åŸºäºæ ‡è®°çš„ç›¸ä¼¼æ€§

- è¿˜æœ‰[æ›´å¤š](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction)ï¼

> éšç€æ—¶é—´çš„æ¨ç§»ï¼Œé€šå¸¸ä¼šæ£€ç´¢å®ä½“ï¼ˆç”¨æˆ·ã€é¡¹ç›®ç­‰ï¼‰çš„ç‰¹å¾å€¼ï¼Œå¹¶åœ¨ä¸åŒé¡¹ç›®ä¸­é‡ç”¨ç›¸åŒçš„ç‰¹å¾ã€‚ä¸ºç¡®ä¿æ£€ç´¢åˆ°æ­£ç¡®çš„ç‰¹å¾å€¼å¹¶é¿å…é‡å¤å·¥ä½œï¼Œå¯ä»¥ä½¿ç”¨[ç‰¹å¾å­˜å‚¨](https://madewithml.com/courses/mlops/feature-store/)ã€‚

>  ç»´åº¦çš„è¯…å’’
> 
> å¦‚æœä¸€ä¸ªç‰¹å¾æœ‰å¾ˆå¤šå”¯ä¸€å€¼ä½†æ¯ä¸ªå”¯ä¸€å€¼éƒ½æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹ï¼ˆä¾‹å¦‚ URL ä½œä¸ºç‰¹å¾ï¼‰ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ
> 
> æ˜¾ç¤ºç­”æ¡ˆ
> 
> å¯ä»¥ä½¿ç”¨[æ•£åˆ—](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html)æˆ–ä½¿ç”¨å®ƒçš„å±æ€§è€Œä¸æ˜¯ç¡®åˆ‡çš„å®ä½“æœ¬èº«æ¥å¯¹æ•°æ®è¿›è¡Œç¼–ç ã€‚ä¾‹å¦‚ï¼Œé€šè¿‡ç”¨æˆ·çš„ä½ç½®å’Œæ”¶è—å¤¹æ¥è¡¨ç¤ºç”¨æˆ·è€Œä¸æ˜¯ä½¿ç”¨ä»–ä»¬çš„ç”¨æˆ· IDï¼Œæˆ–è€…ä½¿ç”¨å…¶åŸŸè€Œä¸æ˜¯ç¡®åˆ‡çš„ url æ¥è¡¨ç¤ºç½‘é¡µã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°å‡å°‘äº†ç‹¬ç‰¹ç‰¹å¾å€¼çš„æ€»æ•°å¹¶å¢åŠ äº†æ¯ä¸ªç‰¹å¾å€¼çš„æ•°æ®ç‚¹æ•°é‡ã€‚



## åº”ç”¨

å¯¹äºåº”ç”¨ç¨‹åºï¼Œå°†å®æ–½ä¸€äº›ä¸æ•°æ®é›†ç›¸å…³çš„é¢„å¤„ç†æ­¥éª¤ã€‚

### ç‰¹å¾å·¥ç¨‹

å¯ä»¥ç»“åˆç°æœ‰çš„è¾“å…¥ç‰¹å¾æ¥åˆ›å»ºæ–°çš„æœ‰æ„ä¹‰çš„ä¿¡å·ï¼ˆå¸®åŠ©æ¨¡å‹å­¦ä¹ ï¼‰ã€‚ä½†æ˜¯ï¼Œå¦‚æœä¸å¯¹ä¸åŒçš„ç»„åˆè¿›è¡Œç»éªŒæ€§è¯•éªŒï¼Œé€šå¸¸æ²¡æœ‰ç®€å•çš„æ–¹æ³•å¯ä»¥çŸ¥é“æŸäº›ç‰¹å¾ç»„åˆæ˜¯å¦æœ‰å¸®åŠ©ã€‚åœ¨è¿™é‡Œï¼Œå¯ä»¥å°†é¡¹ç›®çš„æ ‡é¢˜å’Œæè¿°åˆ†åˆ«ç”¨ä½œç‰¹å¾ï¼Œä½†ä¼šå°†å®ƒä»¬ç»„åˆèµ·æ¥åˆ›å»ºä¸€ä¸ªè¾“å…¥ç‰¹å¾ã€‚

```
# Input
df["text"] = df.title + " " + df.description

```



### clean

ç”±äºæ­£åœ¨å¤„ç†æ–‡æœ¬æ•°æ®ï¼Œå› æ­¤å¯ä»¥åº”ç”¨ä¸€äº›å¸¸è§çš„æ–‡æœ¬é¢„å¤„ç†æ­¥éª¤ï¼š

`!pip install nltk==3.7 -q`

```
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re

```

```
nltk.download("stopwords")
STOPWORDS = stopwords.words("english")
stemmer = PorterStemmer()

```



```
def clean_text(text, lower=True, stem=False, stopwords=STOPWORDS):
    """Clean raw text."""
    # Lower
    if lower:
        text = text.lower()

    # Remove stopwords
    if len(stopwords):
        pattern = re.compile(r'\b(' + r"|".join(stopwords) + r")\b\s*")
        text = pattern.sub('', text)

    # Spacing and filters
    text = re.sub(
        r"([!\"'#$%&()*\+,-./:;<=>?@\\\[\]^_`{|}~])", r" \1 ", text
    )  # add spacing between objects to be filtered
    text = re.sub("[^A-Za-z0-9]+", " ", text)  # remove non alphanumeric chars
    text = re.sub(" +", " ", text)  # remove multiple spaces
    text = text.strip()  # strip white space at the ends

    # Remove links
    text = re.sub(r"http\S+", "", text)

    # Stemming
    if stem:
        text = " ".join([stemmer.stem(word, to_lowercase=lower) for word in text.split(" ")])

    return text

```

```
# Apply to dataframe
original_df = df.copy()
df.text = df.text.apply(clean_text, lower=True, stem=False)
print (f"{original_df.text.values[0]}\n{df.text.values[0]}")

```

YOLO å’Œ RCNN åœ¨çœŸå®ä¸–ç•Œè§†é¢‘ä¸Šçš„æ¯”è¾ƒå°†ç†è®ºå¸¦å…¥å®éªŒå¾ˆé…·ã€‚å¯ä»¥è½»æ¾åœ°åœ¨ Colab ä¸­è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨å‡ åˆ†é’Ÿå†…æ‰¾åˆ°ç»“æœã€‚
æ¯”è¾ƒ yolo rcnn çœŸå®ä¸–ç•Œè§†é¢‘å¸¦æ¥ç†è®ºå®éªŒå¾ˆé…·å¾ˆå®¹æ˜“è®­ç»ƒæ¨¡å‹ colab æ‰¾åˆ°ç»“æœåˆ†é’Ÿ

> è­¦å‘Š
> 
> å°†å¸Œæœ›åœ¨å®ƒä»¬å˜å¾—æ›´é¢‘ç¹æ—¶å¼•å…¥é¢‘ç‡è¾ƒä½çš„ç‰¹å¾ï¼Œæˆ–è€…ä»¥å·§å¦™çš„æ–¹å¼å¯¹å®ƒä»¬è¿›è¡Œç¼–ç ï¼ˆä¾‹å¦‚åˆ†ç®±ã€æå–ä¸€èˆ¬å±æ€§ã€å¸¸è§çš„ n-gramã€ä½¿ç”¨å…¶ä»–ç‰¹å¾å€¼è¿›è¡Œå¹³å‡ç¼–ç ç­‰ï¼‰ï¼Œä»¥ä¾¿å¯ä»¥å‡è½»ç‰¹å¾å€¼ç»´åº¦é—®é¢˜ï¼Œç›´åˆ°èƒ½å¤Ÿæ”¶é›†æ›´å¤šæ•°æ®ã€‚

### æ›´æ¢æ ‡ç­¾

æ ¹æ®[EDA](https://madewithml.com/courses/mlops/exploratory-data-analysis/)çš„å‘ç°ï¼Œå°†åº”ç”¨å‡ ä¸ªçº¦æŸæ¥æ ‡è®°æ•°æ®ï¼š

- å¦‚æœæ•°æ®ç‚¹æœ‰ç›®å‰ä¸æ”¯æŒçš„æ ‡ç­¾ï¼Œå°†ç”¨`other`
- å¦‚æœæŸä¸ªæ ‡ç­¾æ²¡æœ‰*è¶³å¤Ÿçš„*æ ·æœ¬ï¼Œä¼šå°†å…¶æ›¿æ¢ä¸º`other`

```
import json
# Accepted tags (external constraint)
ACCEPTED_TAGS = ["natural-language-processing", "computer-vision", "mlops", "graph-learning"]# Out of scope (OOS) tags
oos_tags = [item for item in df.tag.unique() if item not in ACCEPTED_TAGS]
oos_tags


```

```
# Samples with OOS tags
oos_indices = df[df.tag.isin(oos_tags)].index
df[df.tag.isin(oos_tags)].head()

```





|     | ID  | åˆ›å»ºäº                 | æ ‡é¢˜                      | æè¿°                   | æ ‡ç­¾   |
| --- | --- | ------------------- | ----------------------- | -------------------- | ---- |
| 3ä¸ª  | 15  | 2020-02-28 23:55:26 | å¾ˆæ£’çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢              | è’™ç‰¹å¡æ´›æ ‘æœç´¢è®ºæ–‡çš„ç²¾é€‰åˆ—è¡¨...... | å¼ºåŒ–å­¦ä¹  |
| 37  | 121 | 2020-03-24 04:56:38 | TensorFlow2 ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹     | deep-rl-tf2 æ˜¯ä¸€ä¸ªå®ç°... | å¼ºåŒ–å­¦ä¹  |
| 67  | 218 | 2020-04-06 11:29:57 | ä½¿ç”¨ TensorFlow2 çš„åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹  | ğŸ³ å„ç§åˆ†å¸ƒå¼èµ„æºçš„å®ç°...     | å¼ºåŒ–å­¦ä¹  |
| 74  | 239 | 2020-04-06 18:39:48 | Prophetï¼šå¤§è§„æ¨¡é¢„æµ‹           | ä¸º...ç”Ÿæˆé«˜è´¨é‡é¢„æµ‹çš„å·¥å…·       | æ—¶é—´åºåˆ— |
| 95  | 277 | 2020-04-07 00:30:33 | å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹                  | è¯¾ç¨‹å­¦ä¹ åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ...       | å¼ºåŒ–å­¦ä¹  |

```
# Replace this tag with "other"
df.tag = df.tag.apply(lambda x: "other" if x in oos_tags else x)
df.iloc[oos_indices].head()

```

|     | ID  | åˆ›å»ºäº                 | æ ‡é¢˜                      | æè¿°                   | æ ‡ç­¾  |
| --- | --- | ------------------- | ----------------------- | -------------------- | --- |
| 3ä¸ª  | 15  | 2020-02-28 23:55:26 | å¾ˆæ£’çš„è’™ç‰¹å¡æ´›æ ‘æœç´¢              | è’™ç‰¹å¡æ´›æ ‘æœç´¢è®ºæ–‡çš„ç²¾é€‰åˆ—è¡¨...... | å…¶ä»–  |
| 37  | 121 | 2020-03-24 04:56:38 | TensorFlow2 ä¸­çš„æ·±åº¦å¼ºåŒ–å­¦ä¹     | deep-rl-tf2 æ˜¯ä¸€ä¸ªå®ç°... | å…¶ä»–  |
| 67  | 218 | 2020-04-06 11:29:57 | ä½¿ç”¨ TensorFlow2 çš„åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹  | ğŸ³ å„ç§åˆ†å¸ƒå¼èµ„æºçš„å®ç°...     | å…¶ä»–  |
| 74  | 239 | 2020-04-06 18:39:48 | Prophetï¼šå¤§è§„æ¨¡é¢„æµ‹           | ä¸º...ç”Ÿæˆé«˜è´¨é‡é¢„æµ‹çš„å·¥å…·       | å…¶ä»–  |
| 95  | 277 | 2020-04-07 00:30:33 | å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹                  | è¯¾ç¨‹å­¦ä¹ åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ...       | å…¶ä»–  |

è¿˜å°†é™åˆ¶æ˜ å°„åˆ°ä»…é«˜äºç‰¹å®šé¢‘ç‡é˜ˆå€¼çš„æ ‡ç­¾ã€‚æ²¡æœ‰è¶³å¤Ÿé¡¹ç›®çš„æ ‡ç­¾å°†æ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬æ¥å»ºæ¨¡å®ƒä»¬çš„å…³ç³»ã€‚

```
# Minimum frequency required for a tag
min_freq = 75
tags = Counter(df.tag.values)

```

```
# Tags that just made / missed the cut
@widgets.interact(min_freq=(0, tags.most_common()[0][1]))
def separate_tags_by_freq(min_freq=min_freq):
    tags_above_freq = Counter(tag for tag in tags.elements()
                                    if tags[tag] >= min_freq)
    tags_below_freq = Counter(tag for tag in tags.elements()
                                    if tags[tag] < min_freq)
    print ("Most popular tags:\n", tags_above_freq.most_common(3))
    print ("\nTags that just made the cut:\n", tags_above_freq.most_common()[-3:])
    print ("\nTags that just missed the cut:\n", tags_below_freq.most_common(3))
)))

```

Most popular tags:
 [('natural-language-processing', 388), ('computer-vision', 356), ('other', 87)]
Tags that just made the cut:
 [('computer-vision', 356), ('other', 87), ('mlops', 79)]
Tags that just missed the cut:
 [('graph-learning', 45)]

```
def filter(tag, include=[]):
    """Determine if a given tag is to be included."""
    if tag not in include:
        tag = None
    return tag

```

```
# Filter tags that have fewer than <min_freq> occurrences
tags_above_freq = Counter(tag for tag in tags.elements()
                          if (tags[tag] >= min_freq))
df.tag = df.tag.apply(filter, include=list(tags_above_freq.keys()))

```

```
# Fill None with other
df.tag = df.tag.fillna("other")

```



### ç¼–ç 

å°†å¯¹è¾“å‡ºæ ‡ç­¾è¿›è¡Œç¼–ç ï¼Œå°†ä¸ºæ¯ä¸ªæ ‡ç­¾åˆ†é…ä¸€ä¸ªå”¯ä¸€ç´¢å¼•ã€‚

```
import numpy as np
import random

```

```
# Get data
X = df.text.to_numpy()
y = df.tag

```



å°†ç¼–å†™è‡ªå·±çš„åŸºäº scikit-learn[å®ç°](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)çš„ LabelEncoder ã€‚èƒ½å¤Ÿä¸ºæƒ³è¦åˆ›å»ºçš„å¯¹è±¡ç¼–å†™å¹²å‡€çš„ç±»æ˜¯ä¸€é¡¹éå¸¸æœ‰ä»·å€¼çš„æŠ€èƒ½ã€‚

```
class LabelEncoder(object):
    """Encode labels into unique indices"""
    def __init__(self, class_to_index={}):
        self.class_to_index = class_to_index or {}  # mutable defaults ;)
        self.index_to_class = {v: k for k, v in self.class_to_index.items()}
        self.classes = list(self.class_to_index.keys())

    def __len__(self):
        return len(self.class_to_index)

    def __str__(self):
        return f"<LabelEncoder(num_classes={len(self)})>"

    def fit(self, y):
        classes = np.unique(y)
        for i, class_ in enumerate(classes):
            self.class_to_index[class_] = i
        self.index_to_class = {v: k for k, v in self.class_to_index.items()}
        self.classes = list(self.class_to_index.keys())
        return self

    def encode(self, y):
        encoded = np.zeros((len(y)), dtype=int)
        for i, item in enumerate(y):
            encoded[i] = self.class_to_index[item]
        return encoded

    def decode(self, y):
        classes = []
        for i, item in enumerate(y):
            classes.append(self.index_to_class[item])
        return classes

    def save(self, fp):
        with open(fp, "w") as fp:
            contents = {"class_to_index": self.class_to_index}
            json.dump(contents, fp, indent=4, sort_keys=False)

    @classmethod
    def load(cls, fp):
        with open(fp, "r") as fp:
            kwargs = json.load(fp=fp)
        return cls(**kwargs)

```



> å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰è£…é¥°å™¨ï¼Œè¯·ä»[Python è¯¾ç¨‹](https://madewithml.com/courses/foundations/python/#methods)`@classmethod`ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚[](https://madewithml.com/courses/foundations/python/#methods)

```
# Encode
label_encoder = LabelEncoder()
label_encoder.fit(y)
num_classes = len(label_encoder)

```

```
label_encoder.class_to_index

```

{'computer-vision': 0,
 'mlops': 1,
 'natural-language-processing': 2,
 'other': 3}

```
label_encoder.index_to_class

```

{0: 'computer-vision',
 1: 'mlops',
 2: 'natural-language-processing',
 3: 'other'}

```
# Encode
label_encoder.encode(["computer-vision", "mlops", "mlops"])

```

array([0, 1, 1])

```
# Decode
label_encoder.decode(np.array([0, 1, 1]))

```

['computer-vision', 'mlops', 'mlops']

```

```

è¦å¯¹è¾“å…¥æ–‡æœ¬ç‰¹å¾è¿›è¡Œçš„è®¸å¤š*è½¬æ¢éƒ½æ˜¯ç‰¹å®šäºæ¨¡å‹çš„ã€‚*ä¾‹å¦‚ï¼Œå¯¹äºç®€å•çš„åŸºçº¿ï¼Œå¯ä»¥åš`label encoding`â†’`tf-idf`è€Œå¯¹äºæ›´å¤æ‚çš„æ¶æ„ï¼Œå¯ä»¥åš`label encoding`â†’Â `one-hot encoding`â†’Â `embeddings`ã€‚å› æ­¤ï¼Œåœ¨å®æ–½[åŸºçº¿](https://madewithml.com/courses/mlops/baselines/)æ—¶ï¼Œå°†åœ¨ä¸‹ä¸€ç»„è¯¾ç¨‹ä¸­ä»‹ç»è¿™äº›å†…å®¹ã€‚

> åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œå°†å¯¹é¢„å¤„ç†åçš„æ•°æ®é›†æ‰§è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)ã€‚ä½†æ˜¯ï¼Œæ­¥éª¤çš„é¡ºåºå¯ä»¥é¢ å€’ï¼Œå…·ä½“å–å†³äºé—®é¢˜çš„å®šä¹‰ç¨‹åº¦ã€‚å¦‚æœä¸ç¡®å®šå¦‚ä½•å‡†å¤‡æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨ EDA æ¥å¼„æ¸…æ¥šï¼Œåä¹‹äº¦ç„¶ã€‚

æœ¬æ–‡ä¸»ä½“æºè‡ªä»¥ä¸‹é“¾æ¥ï¼š
```
@article{madewithml,
    author       = {Goku Mohandas},
    title        = { Made With ML },
    howpublished = {\url{https://madewithml.com/}},
    year         = {2022}
}
```