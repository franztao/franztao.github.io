---
layout:     post
title:      æœºå™¨å­¦ä¹ ç¼–æ’
subtitle:   2022å¹´11æœˆ
date:       2022-11-10
author:     franztao
header-img: post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Orchestration

---

## æœºå™¨å­¦ä¹ ç¼–æ’

___

é€šè¿‡åˆ›å»ºå¯æ‰©å±•çš„ç®¡é“æ¥åˆ›å»ºã€å®‰æ’å’Œç›‘æ§å·¥ä½œæµã€‚

## Intuition

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå·²ç»å°† DataOpsï¼ˆELTã€éªŒè¯ç­‰ï¼‰å’Œ MLOpsï¼ˆä¼˜åŒ–ã€è®­ç»ƒã€è¯„ä¼°ç­‰ï¼‰å·¥ä½œæµå®ç°ä¸º Python å‡½æ•°è°ƒç”¨ã€‚è¿™å¾ˆæœ‰æ•ˆï¼Œå› ä¸ºæ•°æ®é›†æ˜¯é™æ€çš„å¹¶ä¸”å¾ˆå°ã€‚ä½†æ˜¯å½“éœ€è¦ï¼š

- åœ¨æ–°æ•°æ®åˆ°æ¥æ—¶**å®‰æ’è¿™äº›å·¥ä½œæµç¨‹ï¼Ÿ**
- **éšç€æ•°æ®çš„å¢é•¿æ‰©å±•**è¿™äº›å·¥ä½œæµç¨‹ï¼Ÿ
- **å°†**è¿™äº›å·¥ä½œæµç¨‹å…±äº«ç»™ä¸‹æ¸¸åº”ç”¨ç¨‹åºï¼Ÿ
- **ç›‘æ§**è¿™äº›å·¥ä½œæµç¨‹ï¼Ÿ

éœ€è¦å°†ç«¯åˆ°ç«¯ ML ç®¡é“åˆ†è§£ä¸ºå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œç¼–æ’çš„å„ä¸ªå·¥ä½œæµç¨‹ã€‚æœ‰å‡ ç§å·¥å…·å¯ä»¥å¸®åŠ©ï¼Œä¾‹å¦‚[Airflow](https://airflow.apache.org/)ã€[Prefect](https://www.prefect.io/)ã€[Dagster](https://dagster.io/)ã€[Luigi](https://luigi.readthedocs.io/en/stable/)ã€[Orchest](https://www.orchest.io/)ï¼Œç”šè‡³ä¸€äº›ä»¥ ML ä¸ºé‡ç‚¹çš„é€‰é¡¹ï¼Œä¾‹å¦‚[Metaflow](https://metaflow.org/)ã€[Flyte](https://flyte.org/)ã€[KubeFlow Pipelines](https://www.kubeflow.org/docs/components/pipelines/overview/pipelines-overview/)ã€[Vertex pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction)ç­‰ã€‚å°†ä½¿ç”¨ AirFlow åˆ›å»ºå·¥ä½œæµç¨‹å¯¹äºå®ƒï¼š

- ä¸šç•Œå¹¿æ³›é‡‡ç”¨å¼€æºå¹³å°
- åŸºäº Python çš„è½¯ä»¶å¼€å‘å·¥å…·åŒ… (SDK)
- æœ¬åœ°è¿è¡Œå’Œè½»æ¾æ‰©å±•çš„èƒ½åŠ›
- å¤šå¹´æ¥çš„æˆç†Ÿåº¦å’Œ apache ç”Ÿæ€ç³»ç»Ÿçš„ä¸€éƒ¨åˆ†

> å°†åœ¨æœ¬åœ°è¿è¡Œ Airflowï¼Œä½†å¯ä»¥é€šè¿‡åœ¨æ‰˜ç®¡é›†ç¾¤å¹³å°ä¸Šè¿è¡Œæ¥è½»æ¾æ‰©å±•å®ƒï¼Œåœ¨è¯¥å¹³å°ä¸Šï¼Œå¯ä»¥åœ¨å¤§å‹æ‰¹å¤„ç†ä½œä¸šï¼ˆAWS [EMR](https://aws.amazon.com/emr/)ã€Google Cloud çš„[Dataproc](https://cloud.google.com/dataproc)ã€æœ¬åœ°ç¡¬ä»¶ã€ ETCã€‚ï¼‰ã€‚

## Airflow

åœ¨åˆ›å»ºç‰¹å®šç®¡é“ä¹‹å‰ï¼Œè®©äº†è§£å’Œå®æ–½[Airflow](https://airflow.apache.org/)çš„æ€»ä½“æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µå°†ä½¿èƒ½å¤Ÿâ€œåˆ›ä½œã€å®‰æ’å’Œç›‘æ§å·¥ä½œæµç¨‹â€ã€‚

> å•ç‹¬çš„å­˜å‚¨åº“
> 
> åœ¨æœ¬è¯¾ä¸­çš„å·¥ä½œå°†å­˜åœ¨äºä¸€ä¸ªå•ç‹¬çš„å­˜å‚¨åº“ä¸­ï¼Œå› æ­¤åˆ›å»ºä¸€ä¸ª`mlops-course`åä¸º`data-engineering`. æœ¬è¯¾çš„æ‰€æœ‰å·¥ä½œéƒ½å¯ä»¥åœ¨ [æ•°æ®å·¥ç¨‹](https://github.com/GokuMohandas/data-engineering)repositoryã€‚

### è®¾ç½®

è¦å®‰è£…å’Œè¿è¡Œ Airflowï¼Œå¯ä»¥åœ¨[æœ¬åœ°](https://airflow.apache.org/docs/apache-airflow/stable/start/local.html)æˆ–ä½¿ç”¨[Docker](https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html)è¿›è¡Œã€‚å¦‚æœ`docker-compose`ç”¨äºåœ¨ Docker å®¹å™¨ä¸­è¿è¡Œ Airflowï¼Œéœ€è¦åˆ†é…è‡³å°‘ 4 GB çš„å†…å­˜ã€‚

```
# Configurations
export AIRFLOW_HOME=${PWD}/airflow
AIRFLOW_VERSION=2.3.3
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"

# Install Airflow (may need to upgrade pip)
pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"

# Initialize DB (SQLite by default)
airflow db init
```

è¿™å°†åˆ›å»ºä¸€ä¸ª`airflow`åŒ…å«ä»¥ä¸‹ç»„ä»¶çš„ç›®å½•ï¼š

```
airflow/
â”œâ”€â”€ logs/
â”œâ”€â”€ airflow.cfg
â”œâ”€â”€ airflow.db
â”œâ”€â”€ unittests.cfg
â””â”€â”€ webserver_config.py
```

å°†ç¼–è¾‘[airflow.cfg](https://github.com/GokuMohandas/data-engineering/blob/main/airflow/airflow.cfg)æ–‡ä»¶ä»¥æœ€é€‚åˆéœ€æ±‚ï¼š

```
# Inside airflow.cfg
enable_xcom_pickling = True  # needed for Great Expectations airflow provider
load_examples = False  # don't clutter webserver with examples
```

å°†æ‰§è¡Œé‡ç½®ä»¥å®ç°è¿™äº›é…ç½®æ›´æ”¹ã€‚

ç°åœ¨å·²ç»å‡†å¤‡å¥½ä½¿ç”¨ç®¡ç†å‘˜ç”¨æˆ·æ¥åˆå§‹åŒ–æ•°æ®åº“ï¼Œå°†ä½¿ç”¨å®ƒæ¥ç™»å½•ä»¥è®¿é—®åœ¨ç½‘ç»œæœåŠ¡å™¨ä¸­çš„å·¥ä½œæµã€‚

```
# We'll be prompted to enter a password
airflow users create \
    --username admin \
    --firstname FIRSTNAME \
    --lastname LASTNAME \
    --role Admin \
    --email EMAIL
```

### ç½‘ç»œæœåŠ¡å™¨

åˆ›å»ºç”¨æˆ·åï¼Œå°±å¯ä»¥å¯åŠ¨ç½‘ç»œæœåŠ¡å™¨å¹¶ä½¿ç”¨å‡­æ®ç™»å½•äº†ã€‚

```
# Launch webserver
source venv/bin/activate
export AIRFLOW_HOME=${PWD}/airflow
airflow webserver --port 8080  # http://localhost:8080
```

ç½‘ç»œæœåŠ¡å™¨å…è®¸é€šè¿‡ UI è¿è¡Œå’Œæ£€æŸ¥å·¥ä½œæµç¨‹ï¼Œå»ºç«‹ä¸å¤–éƒ¨æ•°æ®å­˜å‚¨ã€ç®¡ç†ç”¨æˆ·ç­‰çš„è¿æ¥ã€‚åŒæ ·ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Airflow çš„[REST API](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html)æˆ–[å‘½ä»¤è¡Œç•Œé¢ (CLI)](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html)æ¥æ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚ä½†æ˜¯ï¼Œå°†ä½¿ç”¨ç½‘ç»œæœåŠ¡å™¨ï¼Œå› ä¸ºå®ƒå¯ä»¥æ–¹ä¾¿åœ°ç›´è§‚åœ°æ£€æŸ¥å·¥ä½œæµç¨‹ã€‚

![æ°”æµç½‘ç»œæœåŠ¡å™¨](https://madewithml.com/static/images/mlops/orchestration/webserver.png)

åœ¨äº†è§£ Airflow å¹¶å®æ–½å·¥ä½œæµç¨‹æ—¶ï¼Œå°†æ¢ç´¢ç½‘ç»œæœåŠ¡å™¨çš„ä¸åŒç»„ä»¶ã€‚

### è°ƒåº¦å™¨

æ¥ä¸‹æ¥ï¼Œéœ€è¦å¯åŠ¨è°ƒåº¦ç¨‹åºï¼Œå®ƒå°†æ‰§è¡Œå’Œç›‘æ§å·¥ä½œæµç¨‹ä¸­çš„ä»»åŠ¡ã€‚è¯¥è®¡åˆ’é€šè¿‡ä»å…ƒæ•°æ®æ•°æ®åº“ä¸­è¯»å–æ¥æ‰§è¡Œä»»åŠ¡ï¼Œå¹¶ç¡®ä¿ä»»åŠ¡å…·æœ‰å®Œæˆè¿è¡Œæ‰€éœ€çš„å†…å®¹ã€‚å°†ç»§ç»­åœ¨_å•ç‹¬çš„ç»ˆç«¯_çª—å£ä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```
# Launch scheduler (in separate terminal)
source venv/bin/activate
export AIRFLOW_HOME=${PWD}/airflow
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
airflow scheduler
```

### æ‰§è¡Œè€…

å½“è°ƒåº¦ç¨‹åºä»å…ƒæ•°æ®æ•°æ®åº“ä¸­è¯»å–æ•°æ®æ—¶ï¼Œæ‰§è¡Œç¨‹åºä¼šç¡®å®šä»»åŠ¡è¿è¡Œå®Œæˆæ‰€éœ€çš„å·¥ä½œè¿›ç¨‹ã€‚ç”±äºé»˜è®¤æ•°æ®åº“ SQLlite ä¸æ”¯æŒå¤šä¸ªè¿æ¥ï¼Œå› æ­¤é»˜è®¤æ‰§è¡Œå™¨æ˜¯[Sequential Executor](https://airflow.apache.org/docs/apache-airflow/stable/executor/sequential.html)ã€‚ä½†æ˜¯ï¼Œå¦‚æœé€‰æ‹©ç”Ÿäº§çº§çš„æ•°æ®åº“é€‰é¡¹ï¼Œä¾‹å¦‚ PostgresSQL æˆ– MySQLï¼Œå¯ä»¥é€‰æ‹©å¯æ‰©å±•çš„[Executor åç«¯](https://airflow.apache.org/docs/apache-airflow/stable/executor/index.html#supported-backends)Celeryã€Kubernetes ç­‰ã€‚ä¾‹å¦‚ï¼Œ[ä½¿ç”¨ Docker è¿è¡Œ Airflow](https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html)ä½¿ç”¨ PostgresSQL ä½œä¸ºæ•°æ®åº“ï¼Œå› æ­¤ä½¿ç”¨ Celery Executor åç«¯å¹¶è¡Œè¿è¡Œä»»åŠ¡ã€‚

## æœ‰å‘æ— ç¯å›¾

å·¥ä½œæµç”±æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰å®šä¹‰ï¼Œå…¶èŠ‚ç‚¹ä»£è¡¨ä»»åŠ¡ï¼Œè¾¹ä»£è¡¨ä»»åŠ¡ä¹‹é—´çš„æ•°æ®æµå…³ç³»ã€‚ç›´æ¥å’Œéå¾ªç¯æ„å‘³ç€å·¥ä½œæµåªèƒ½åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šæ‰§è¡Œï¼Œå¹¶ä¸”ä¸€æ—¦ä¸‹æ¸¸ä»»åŠ¡å¼€å§‹ï¼Œä¹‹å‰çš„ä¸Šæ¸¸ä»»åŠ¡å°±ä¸èƒ½å†æ¬¡è¿è¡Œã€‚

![åŸºæœ¬ DAG](https://madewithml.com/static/images/mlops/orchestration/basic_dag.png)

DAG å¯ä»¥åœ¨ç›®å½•å†…çš„ Python å·¥ä½œæµè„šæœ¬ä¸­å®šä¹‰`airflow/dags`ï¼Œå®ƒä»¬ä¼šè‡ªåŠ¨å‡ºç°ï¼ˆå¹¶ä¸æ–­æ›´æ–°ï¼‰åœ¨ç½‘ç»œæœåŠ¡å™¨ä¸Šã€‚[åœ¨å¼€å§‹åˆ›å»º DataOps å’Œ MLOps å·¥ä½œæµä¹‹å‰ï¼Œå°†é€šè¿‡æ°”æµ/dags/example.py](https://github.com/GokuMohandas/data-engineering/blob/main/airflow/dags/example.py)ä¸­æ¦‚è¿°çš„ç¤ºä¾‹ DAG äº†è§£ Airflow çš„æ¦‚å¿µã€‚åœ¨æ–°çš„ï¼ˆç¬¬ä¸‰ä¸ªï¼‰ç»ˆç«¯çª—å£ä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```
mkdir airflow/dags
touch airflow/dags/example.py
```

åœ¨æ¯ä¸ªå·¥ä½œæµè„šæœ¬ä¸­ï¼Œå¯ä»¥å®šä¹‰ä¸€äº›é»˜è®¤å‚æ•°ï¼Œè¿™äº›å‚æ•°å°†åº”ç”¨äºè¯¥å·¥ä½œæµä¸­çš„æ‰€æœ‰ DAGã€‚

```
# Default DAG args
default_args = {
    "owner": "airflow",
}
```

> é€šå¸¸ï¼Œ DAG å¹¶ä¸æ˜¯ Airflow é›†ç¾¤ä¸­å”¯ä¸€è¿è¡Œçš„ DAGã€‚ä½†æ˜¯ï¼Œå½“éœ€è¦ä¸åŒçš„èµ„æºã€åŒ…ç‰ˆæœ¬ç­‰æ—¶ï¼Œæ‰§è¡Œä¸åŒçš„å·¥ä½œæµå¯èƒ½ä¼šå¾ˆæ··ä¹±ï¼Œæœ‰æ—¶ç”šè‡³æ˜¯ä¸å¯èƒ½çš„ã€‚å¯¹äºæ‹¥æœ‰å¤šä¸ªé¡¹ç›®çš„å›¢é˜Ÿæ¥è¯´ï¼Œä½¿ç”¨ KubernetesPodOperator ä¹‹ç±»çš„ä¸œè¥¿ä½¿ç”¨éš”ç¦»çš„[docker æ˜ åƒ](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes)æ¥æ‰§è¡Œæ¯ä¸ªä½œä¸šæ˜¯ä¸ªå¥½ä¸»æ„ã€‚

å¯ä»¥ä½¿ç”¨è®¸å¤š[å‚æ•°](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html#airflow.models.dag.DAG)ï¼ˆå°†è¦†ç›– ä¸­çš„ç›¸åŒå‚æ•°`default_args`ï¼‰å’Œå‡ ç§ä¸åŒçš„æ–¹å¼åˆå§‹åŒ– DAGï¼š

- ä½¿ç”¨[with è¯­å¥](https://docs.python.org/3/reference/compound_stmts.html#the-with-statement)
  
  ```
  from airflow import DAG
  
  with DAG(
      dag_id="example",
      description="Example DAG",
      default_args=default_args,
      schedule_interval=None,
      start_date=days_ago(2),
      tags=["example"],
  ) as example:
      # Define tasks
      pass
  ```

- ä½¿ç”¨[dag è£…é¥°å™¨](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#dag-decorator)
  
  ```
  from airflow.decorators import dag
  
  @dag(
      dag_id="example",
      description="Example DAG",
      default_args=default_args,
      schedule_interval=None,
      start_date=days_ago(2),
      tags=["example"],
  )
  def example():
      # Define tasks
      pass
  ```

> å¯ä»¥ä½¿ç”¨è®¸å¤š[å‚æ•°](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html#airflow.models.dag.DAG)æ¥åˆå§‹åŒ– DAGï¼ŒåŒ…æ‹¬ a`start_date`å’Œ a `schedule_interval`ã€‚è™½ç„¶å¯ä»¥è®©å·¥ä½œæµæŒ‰æ—¶é—´èŠ‚å¥æ‰§è¡Œï¼Œä½†è®¸å¤š ML å·¥ä½œæµæ˜¯ç”±äº‹ä»¶å¯åŠ¨çš„ï¼Œå¯ä»¥ä½¿ç”¨[ä¼ æ„Ÿå™¨](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/index.html)å’Œ[hook](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/hooks/index.html)å°†å…¶æ˜ å°„åˆ°å¤–éƒ¨æ•°æ®åº“ã€æ–‡ä»¶ç³»ç»Ÿç­‰ã€‚

## ä»»åŠ¡

ä»»åŠ¡æ˜¯åœ¨å·¥ä½œæµä¸­æ‰§è¡Œçš„æ“ä½œï¼Œç”± DAG ä¸­çš„èŠ‚ç‚¹è¡¨ç¤ºã€‚æ¯ä¸ªä»»åŠ¡åº”è¯¥æ˜¯ä¸€ä¸ªæ˜ç¡®å®šä¹‰çš„å•ä¸ªæ“ä½œï¼Œå¹¶ä¸”åº”è¯¥æ˜¯å¹‚ç­‰çš„ï¼Œè¿™æ„å‘³ç€å¯ä»¥å¤šæ¬¡æ‰§è¡Œå®ƒå¹¶æœŸæœ›ç›¸åŒçš„ç»“æœå’Œç³»ç»ŸçŠ¶æ€ã€‚å¦‚æœéœ€è¦é‡è¯•å¤±è´¥çš„ä»»åŠ¡å¹¶ä¸”ä¸å¿…æ‹…å¿ƒé‡ç½®ç³»ç»ŸçŠ¶æ€ï¼Œè¿™å¾ˆé‡è¦ã€‚ä¸ DAG ä¸€æ ·ï¼Œæœ‰å‡ ç§ä¸åŒçš„æ–¹å¼æ¥å®ç°ä»»åŠ¡ï¼š

- ä½¿ç”¨[ä»»åŠ¡è£…é¥°å™¨](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#concepts-task-decorator)
  
  ```
  from airflow.decorators import dag, task
  from airflow.utils.dates import days_ago
  
  @dag(
      dag_id="example",
      description="Example DAG with task decorators",
      default_args=default_args,
      schedule_interval=None,
      start_date=days_ago(2),
      tags=["example"],
  )
  def example():
      @task
      def task_1():
          return 1
      @task
      def task_2(x):
          return x+1
  ```

- ä½¿ç”¨[è¿ç®—ç¬¦](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html)
  
  ```
  from airflow.decorators import dag
  from airflow.operators.bash_operator import BashOperator
  from airflow.utils.dates import days_ago
  
  @dag(
      dag_id="example",
      description="Example DAG with Operators",
      default_args=default_args,
      schedule_interval=None,
      start_date=days_ago(2),
      tags=["example"],
  )
  def example():
      # Define tasks
      task_1 = BashOperator(task_id="task_1", bash_command="echo 1")
      task_2 = BashOperator(task_id="task_2", bash_command="echo 2")
  ```

> è™½ç„¶å›¾æ˜¯æœ‰å‘çš„ï¼Œä½†å¯ä»¥ä¸ºæ¯ä¸ªä»»åŠ¡å»ºç«‹ä¸€å®šçš„[è§¦å‘è§„åˆ™](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#trigger-rules)ï¼Œä»¥åœ¨çˆ¶ä»»åŠ¡çš„æ¡ä»¶æˆåŠŸæˆ–å¤±è´¥æ—¶æ‰§è¡Œã€‚

### Operators

ç¬¬ä¸€ç§åˆ›å»ºä»»åŠ¡çš„æ–¹æ³•æ¶‰åŠä½¿ç”¨æ“ä½œç¬¦ï¼Œå®ƒå®šä¹‰äº†ä»»åŠ¡å°†è¦åšä»€ä¹ˆã€‚Airflow æœ‰å¾ˆå¤šå†…ç½®çš„ Operatorï¼Œä¾‹å¦‚[BashOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html#airflow.operators.bash.BashOperator)æˆ–[PythonOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html#airflow.operators.python.PythonOperator)ï¼Œå®ƒä»¬å¯ä»¥è®©åˆ†åˆ«æ‰§è¡Œ bash å’Œ Python å‘½ä»¤ã€‚

```
# BashOperator
from airflow.operators.bash_operator import BashOperator
task_1 = BashOperator(task_id="task_1", bash_command="echo 1")

# PythonOperator
from airflow.operators.python import PythonOperator
task_2 = PythonOperator(
    task_id="task_2",
    python_callable=foo,
    op_kwargs={"arg1": ...})
```

è¿˜æœ‰è®¸å¤šå…¶ä»– Airflow åŸç”Ÿ[Operator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/index.html)ï¼ˆç”µå­é‚®ä»¶ã€S3ã€MySQLã€Hive ç­‰ï¼‰ï¼Œä»¥åŠ[ç¤¾åŒºç»´æŠ¤çš„æä¾›ç¨‹åºåŒ…](https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html)ï¼ˆKubernetesã€Snowflakeã€Azureã€AWSã€Salesforceã€Tableau ç­‰ï¼‰ï¼Œç”¨äºæ‰§è¡Œç‰¹å®šäºæŸäº›å¹³å°æˆ–å·¥å…·ã€‚

> è¿˜å¯ä»¥é€šè¿‡æ‰©å±•[BashOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html#airflow.operators.bash.BashOperator)ç±»æ¥åˆ›å»ºè‡ªå·±çš„[è‡ªå®šä¹‰è¿ç®—ç¬¦](https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html)

### å…³ç³»

ä¸€æ—¦ä½¿ç”¨è¿ç®—ç¬¦æˆ–ä¿®é¥°å‡½æ•°å®šä¹‰äº†ä»»åŠ¡ï¼Œéœ€è¦å®šä¹‰å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼ˆè¾¹ï¼‰ã€‚å®šä¹‰å…³ç³»çš„æ–¹å¼å–å†³äºä»»åŠ¡æ˜¯å¦‚ä½•å®šä¹‰çš„ï¼š

- ä½¿ç”¨è£…é¥°å‡½æ•°
  
  ```
  # Task relationships
  x = task_1()
  y = task_2(x=x)
  ```

- ä½¿ç”¨è¿ç®—ç¬¦
  
  ```
  # Task relationships
  task_1 >> task_2  # same as task_1.set_downstream(task_2) or
                    # task_2.set_upstream(task_1)
  ```

åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œéƒ½å°†`task_2`ä¸‹æ¸¸ä»»åŠ¡è®¾ç½®ä¸º`task_1`.

> note
> 
> ç”šè‡³å¯ä»¥é€šè¿‡ä½¿ç”¨è¿™äº›ç¬¦å·æ¥å®šä¹‰å…³ç³»æ¥åˆ›å»ºå¤æ‚çš„ DAGã€‚
> 
> ```
> task_1 >> [task_2_1, task_2_2] >> task_3
> task_2_2 >> task_4
> [task_3, task_4] >> task_5
> ```

![æœ‰å‘æ— ç¯å›¾](https://madewithml.com/static/images/mlops/orchestration/dag.png)

### XComs

å½“ä½¿ç”¨ä»»åŠ¡è£…é¥°å™¨æ—¶ï¼Œå¯ä»¥çœ‹åˆ°å€¼æ˜¯å¦‚ä½•åœ¨ä»»åŠ¡ä¹‹é—´ä¼ é€’çš„ã€‚ä½†æ˜¯ï¼Œå¦‚ä½•åœ¨ä½¿ç”¨è¿ç®—ç¬¦æ—¶ä¼ é€’å€¼ï¼ŸAirflow ä½¿ç”¨ XComsï¼ˆäº¤å‰é€šä¿¡ï¼‰å¯¹è±¡ï¼Œé€šè¿‡é”®ã€å€¼ã€æ—¶é—´æˆ³å’Œ task\_id å®šä¹‰ï¼Œåœ¨ä»»åŠ¡ä¹‹é—´æ¨é€å’Œæ‹‰å–å€¼ã€‚å½“ä½¿ç”¨ä¿®é¥°å‡½æ•°æ—¶ï¼ŒXComs æ˜¯åœ¨åº•å±‚ä½¿ç”¨çš„ï¼Œä½†å®ƒè¢«æŠ½è±¡æ‰äº†ï¼Œå…è®¸åœ¨ Python å‡½æ•°ä¹‹é—´æ— ç¼åœ°ä¼ é€’å€¼ã€‚ä½†æ˜¯åœ¨ä½¿ç”¨è¿ç®—ç¬¦æ—¶ï¼Œéœ€è¦æ ¹æ®éœ€è¦æ˜¾å¼åœ°æ¨é€å’Œæ‹‰å–å€¼ã€‚

```
def _task_1(ti):
    x = 2
    ti.xcom_push(key="x", value=x)

def _task_2(ti):
    x = ti.xcom_pull(key="x", task_ids=["task_1"])[0]
    y = x + 3
    ti.xcom_push(key="y", value=y)

@dag(
    dag_id="example",
    description="Example DAG",
    default_args=default_args,
    schedule_interval=None,
    start_date=days_ago(2),
    tags=["example"],
)
def example2():
    # Tasks
    task_1 = PythonOperator(task_id="task_1", python_callable=_task_1)
    task_2 = PythonOperator(task_id="task_2", python_callable=_task_2)
    task_1 >> task_2
```

è¿˜å¯ä»¥é€šè¿‡è½¬åˆ°**Admin** >> **XComs åœ¨ç½‘ç»œæœåŠ¡å™¨ä¸ŠæŸ¥çœ‹ XCom**ï¼š

![xcoms](https://madewithml.com/static/images/mlops/orchestration/xcoms.png)

> warning
> 
> åœ¨ä»»åŠ¡ä¹‹é—´ä¼ é€’çš„æ•°æ®åº”è¯¥å¾ˆå°ï¼ˆå…ƒæ•°æ®ã€æŒ‡æ ‡ç­‰ï¼‰ï¼Œå› ä¸º Airflow çš„å…ƒæ•°æ®æ•°æ®åº“æ— æ³•å®¹çº³å¤§å‹å·¥ä»¶ã€‚ä½†æ˜¯ï¼Œå¦‚æœç¡®å®éœ€è¦å­˜å‚¨å’Œä½¿ç”¨ä»»åŠ¡çš„å¤§é‡ç»“æœï¼Œæœ€å¥½ä½¿ç”¨å¤–éƒ¨æ•°æ®å­˜å‚¨ï¼ˆåšå®¢å­˜å‚¨ã€æ¨¡å‹æ³¨å†Œè¡¨ç­‰ï¼‰å¹¶ä½¿ç”¨ Spark æˆ–å†…éƒ¨æ•°æ®ç³»ç»Ÿï¼ˆå¦‚æ•°æ®ä»“åº“ï¼‰æ‰§è¡Œç¹é‡çš„å¤„ç†ã€‚

## DAG è¿è¡Œ

ä¸€æ—¦å®šä¹‰äº†ä»»åŠ¡åŠå…¶å…³ç³»ï¼Œå°±å¯ä»¥è¿è¡Œ DAGã€‚å°†å¼€å§‹åƒè¿™æ ·å®šä¹‰ DAGï¼š

```
# Run DAGs
example1_dag = example_1()
example2_dag = example_2()
```

å½“åˆ·æ–°[Airflow ç½‘ç»œæœåŠ¡å™¨](http://localhost:8080/)æ—¶ï¼Œæ–°çš„ DAG å°±ä¼šå‡ºç°ã€‚

### æ‰‹åŠ¨çš„

 DAG æœ€åˆæ˜¯æš‚åœçš„ï¼Œå› ä¸º`dags_are_paused_at_creation = True`åœ¨[airflow.cfg](https://github.com/GokuMohandas/data-engineering/blob/main/airflow/airflow.cfg)é…ç½®ä¸­æŒ‡å®šï¼Œæ‰€ä»¥å¿…é¡»é€šè¿‡å•å‡»å®ƒ > å–æ¶ˆæš‚åœå®ƒï¼ˆåˆ‡æ¢ï¼‰> è§¦å‘å®ƒï¼ˆæŒ‰é’®ï¼‰æ¥æ‰‹åŠ¨æ‰§è¡Œè¿™ä¸ª DAGã€‚è¦æŸ¥çœ‹ DAG è¿è¡Œä¸­ä»»ä½•ä»»åŠ¡çš„æ—¥å¿—ï¼Œå¯ä»¥å•å‡»ä»»åŠ¡ > æ—¥å¿—ã€‚

![è§¦å‘ DAG](https://madewithml.com/static/images/mlops/orchestration/trigger.png)

> note
> 
> è¿˜å¯ä»¥ä½¿ç”¨ Airflow çš„[REST API](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html)ï¼ˆå°†[é…ç½®æˆæƒ](https://airflow.apache.org/docs/apache-airflow/stable/security/api.html)ï¼‰æˆ–[å‘½ä»¤è¡Œç•Œé¢ (CLI)](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html)æ¥æ£€æŸ¥å’Œè§¦å‘å·¥ä½œæµï¼ˆä»¥åŠæ›´å¤šï¼‰ã€‚æˆ–è€…ç”šè‡³å¯ä»¥ä½¿ç”¨[`trigger_dagrun`](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/trigger_dagrun/index.html)Operator ä»å¦ä¸€ä¸ªå·¥ä½œæµç¨‹ä¸­è§¦å‘ DAGã€‚
> 
> ```
> # CLI to run dags
> airflow dags trigger <DAG_ID>
> ```

### é—´éš”

å¦‚æœåœ¨å®šä¹‰ DAG æ—¶æŒ‡å®šäº†ä¸€ä¸ª`start_date`and `schedule_interval`ï¼Œå®ƒå°†åœ¨é€‚å½“çš„æ—¶é—´è‡ªåŠ¨æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„ DAG å°†åœ¨ä¸¤å¤©å‰å¼€å§‹ï¼Œå¹¶å°†åœ¨æ¯å¤©å¼€å§‹æ—¶è§¦å‘ã€‚

```
from airflow.decorators import dag
from airflow.utils.dates import days_ago
from datetime import timedelta

@dag(
    dag_id="example",
    default_args=default_args,
    schedule_interval=timedelta(days=1),
    start_date=days_ago(2),
    tags=["example"],
    catch_up=False,
)
```

> warning
> 
> æ ¹æ®`start_date`and `schedule_interval`ï¼Œå·¥ä½œæµç¨‹åº”è¯¥å·²ç»è¢«è§¦å‘äº†å‡ æ¬¡ï¼ŒAirflow å°†å°è¯•[èµ¶ä¸Š](https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#catchup)å½“å‰æ—¶é—´ã€‚`catchup=False`å¯ä»¥é€šè¿‡åœ¨å®šä¹‰ DAG æ—¶è¿›è¡Œè®¾ç½®æ¥é¿å…è¿™ç§æƒ…å†µã€‚è¿˜å¯ä»¥å°†æ­¤é…ç½®è®¾ç½®ä¸ºé»˜è®¤å‚æ•°çš„ä¸€éƒ¨åˆ†ï¼š
> 
> ```
> default_args = {
>     "owner": "airflow",
>     "catch_up": False,
> }
> ```
> 
> ä½†æ˜¯ï¼Œå¦‚æœç¡®å®æƒ³åœ¨è¿‡å»è¿è¡Œç‰¹å®šçš„è¿è¡Œï¼Œå¯ä»¥æ‰‹åŠ¨[å›å¡«](https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#backfill)éœ€è¦çš„å†…å®¹ã€‚

è¿˜å¯ä»¥ä¸ºå‚æ•°æŒ‡å®šä¸€ä¸ª[cron](https://crontab.guru/)è¡¨è¾¾å¼ï¼Œ`schedule_interval`ç”šè‡³å¯ä»¥ä½¿ç”¨[cron é¢„è®¾](https://airflow.apache.org/docs/apache-airflow/stable/dag-run.html#cron-presets)ã€‚

> Airflow çš„è°ƒåº¦ç¨‹åº`schedule_interval`å°†ä»`start_date`. ä¾‹å¦‚ï¼Œå¦‚æœå¸Œæœ›å·¥ä½œæµå¼€å§‹`01-01-1983`å¹¶è¿è¡Œ`@daily`ï¼Œé‚£ä¹ˆç¬¬ä¸€æ¬¡è¿è¡Œå°†ç«‹å³åœ¨`01-01-1983T11:59`.

### ä¼ æ„Ÿå™¨

è™½ç„¶åœ¨è®¡åˆ’çš„æ—¶é—´é—´éš”å†…æ‰§è¡Œè®¸å¤šæ•°æ®å¤„ç†å·¥ä½œæµå¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ï¼Œä½†æœºå™¨å­¦ä¹ å·¥ä½œæµå¯èƒ½éœ€è¦æ›´ç»†å¾®çš„è§¦å‘å™¨ã€‚ä¸åº”è¯¥é€šè¿‡è¿è¡Œå·¥ä½œæµæ¥æµªè´¹è®¡ç®—ï¼Œ_ä»¥é˜²ä¸‡ä¸€_æœ‰æ–°æ•°æ®ã€‚ç›¸åï¼Œå¯ä»¥ä½¿ç”¨[ä¼ æ„Ÿå™¨](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/)åœ¨æ»¡è¶³æŸäº›å¤–éƒ¨æ¡ä»¶æ—¶è§¦å‘å·¥ä½œæµã€‚ä¾‹å¦‚ï¼Œå½“æ•°æ®åº“ä¸­å‡ºç°æ–°ä¸€æ‰¹å¸¦æ³¨é‡Šçš„æ•°æ®æˆ–æ–‡ä»¶ç³»ç»Ÿä¸­å‡ºç°ç‰¹å®šæ–‡ä»¶æ—¶ï¼Œå¯ä»¥å¯åŠ¨æ•°æ®å¤„ç†ç­‰ã€‚

> Airflow è¿˜æœ‰å¾ˆå¤šå…¶ä»–åŠŸèƒ½ï¼ˆç›‘æ§ã€ä»»åŠ¡ç»„ã€æ™ºèƒ½ä¼ æ„Ÿå™¨ç­‰ï¼‰ï¼Œå› æ­¤è¯·åŠ¡å¿…åœ¨éœ€è¦æ—¶ä½¿ç”¨[å®˜æ–¹æ–‡æ¡£](https://airflow.apache.org/docs/apache-airflow/stable/index.html)è¿›è¡Œæ¢ç´¢ã€‚

## æ•°æ®è¿ç»´

ç°åœ¨å·²ç»å›é¡¾äº† Airflow çš„ä¸»è¦æ¦‚å¿µï¼Œå·²ç»å‡†å¤‡å¥½åˆ›å»º DataOps å·¥ä½œæµäº†ã€‚[è¿™ä¸åœ¨æ•°æ®å †æ ˆè¯¾ç¨‹](https://franztao.github.io/2022/11/10/Data_stack/)ä¸­å®šä¹‰çš„å·¥ä½œæµç¨‹å®Œå…¨ç›¸åŒâ€”â€”æå–ã€åŠ è½½å’Œè½¬æ¢â€”â€”ä½†è¿™æ¬¡å°†ä»¥ç¼–ç¨‹æ–¹å¼å®Œæˆæ‰€æœ‰å·¥ä½œå¹¶ä½¿ç”¨ Airflow è¿›è¡Œç¼–æ’ã€‚

![ELT](https://madewithml.com/static/images/mlops/testing/production.png)

å°†ä»åˆ›å»ºå®šä¹‰å·¥ä½œæµç¨‹çš„è„šæœ¬å¼€å§‹ï¼š

```
touch airflow/dags/workflows.py
```

```
from pathlib import Path
from airflow.decorators import dag
from airflow.utils.dates import days_ago

# Default DAG args
default_args = {
    "owner": "airflow",
    "catch_up": False,
}
BASE_DIR = Path(__file__).parent.parent.parent.absolute()

@dag(
    dag_id="dataops",
    description="DataOps workflows.",
    default_args=default_args,
    schedule_interval=None,
    start_date=days_ago(2),
    tags=["dataops"],
)
def dataops():
    """DataOps workflows."""
    pass

# Run DAG
do = dataops()
```

åœ¨ä¸¤ä¸ªå•ç‹¬çš„ç»ˆç«¯ä¸­ï¼Œæ¿€æ´»è™šæ‹Ÿç¯å¢ƒå¹¶å¯åŠ¨ Airflow ç½‘ç»œæœåŠ¡å™¨å’Œè°ƒåº¦ç¨‹åºï¼š

```
# Airflow webserver

source venv/bin/activate
export AIRFLOW_HOME=${PWD}/airflow
export GOOGLE_APPLICATION_CREDENTIALS=/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE
airflow webserver --port 8080

# Go to http://localhost:8080
```

```
# Airflow scheduler

source venv/bin/activate
export AIRFLOW_HOME=${PWD}/airflow
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
export GOOGLE_APPLICATION_CREDENTIALS=~/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE
airflow scheduler
```

### æå–å’ŒåŠ è½½

å°†ä½¿ç”¨åœ¨[æ•°æ®å †æ ˆè¯¾ç¨‹](https://franztao.github.io/2022/11/10/Data_stack/)ä¸­è®¾ç½®çš„ Airbyte è¿æ¥ï¼Œä½†è¿™æ¬¡å°†ä»¥ç¼–ç¨‹æ–¹å¼è§¦å‘ä¸ Airflow çš„æ•°æ®åŒæ­¥ã€‚é¦–å…ˆï¼Œè®©ç¡®ä¿ Airbyte åœ¨å…¶å­˜å‚¨åº“ä¸­çš„å•ç‹¬ç»ˆç«¯ä¸Šè¿è¡Œï¼š

```
git clone https://github.com/airbytehq/airbyte.git  # skip if already create in data-stack lesson
cd airbyte
docker-compose up
```

æ¥ä¸‹æ¥ï¼Œè®©å®‰è£…æ‰€éœ€çš„åŒ…å¹¶å»ºç«‹ Airbyte å’Œ Airflow ä¹‹é—´çš„è¿æ¥ï¼š

```
pip install apache-airflow-providers-airbyte==3.1.0
```

1. è½¬åˆ°[Airflow ç½‘ç»œæœåŠ¡å™¨](http://localhost:8080/)å¹¶å•å‡»`Admin`>Â `Connections`> â•

2. æ·»åŠ å…·æœ‰ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯çš„è¿æ¥ï¼š
   
   ```
   Connection ID: airbyte
   Connection Type: HTTP
   Host: localhost
   Port: 8000
   ```

> ä¹Ÿå¯ä»¥é€šè¿‡[ç¼–ç¨‹](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#connection-cli)æ–¹å¼å»ºç«‹è¿æ¥ï¼Œä½†æœ€å¥½ä½¿ç”¨ UI æ¥äº†è§£å¼•æ“ç›–ä¸‹å‘ç”Ÿçš„äº‹æƒ…ã€‚

ä¸ºäº†æ‰§è¡Œtransformersæå–å’ŒåŠ è½½æ•°æ®åŒæ­¥ï¼Œå¯ä»¥ä½¿ç”¨[`AirbyteTriggerSyncOperator`](https://airflow.apache.org/docs/apache-airflow-providers-airbyte/stable/operators/airbyte.html)ï¼š

```
@dag(...)
def dataops():
    """Production DataOps workflows."""
    # Extract + Load
    extract_and_load_projects = AirbyteTriggerSyncOperator(
        task_id="extract_and_load_projects",
        airbyte_conn_id="airbyte",
        connection_id="XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX",  # REPLACE
        asynchronous=False,
        timeout=3600,
        wait_seconds=3,
    )
    extract_and_load_tags = AirbyteTriggerSyncOperator(
        task_id="extract_and_load_tags",
        airbyte_conn_id="airbyte",
        connection_id="XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX",  # REPLACE
        asynchronous=False,
        timeout=3600,
        wait_seconds=3,
    )

    # Define DAG
    extract_and_load_projects
    extract_and_load_tags
```

å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰¾åˆ°`connection_id`æ¯ä¸ª Airbyte è¿æ¥ï¼š

1. è½¬åˆ°transformers[Airbyte ç½‘ç»œæœåŠ¡å™¨](http://localhost:8000/)å¹¶å•å‡»`Connections`å·¦ä¾§èœå•ã€‚

2. å•å‡»è¦ä½¿ç”¨çš„ç‰¹å®šè¿æ¥ï¼ŒURL åº”å¦‚ä¸‹æ‰€ç¤ºï¼š
   
   ```
   https://demo.airbyte.io/workspaces/<WORKSPACE_ID>/connections/<CONNECTION_ID>/status
   ```

3. ä½ç½®ä¸­çš„å­—ç¬¦ä¸²`CONNECTION_ID`æ˜¯è¿æ¥çš„ IDã€‚

ç°åœ¨å¯ä»¥è§¦å‘transformers DAG å¹¶æŸ¥çœ‹æå–çš„æ•°æ®åŠ è½½åˆ°transformers BigQuery æ•°æ®ä»“åº“ä¸­ï¼Œä½†æ˜¯ä¸€æ—¦å®šä¹‰äº†æ•´ä¸ª DataOps å·¥ä½œæµï¼Œå°†ç»§ç»­å¼€å‘å’Œæ‰§è¡Œtransformers DAGã€‚

### è¯å®

å¯ä»¥å®šåˆ¶æå–æ•°æ®çš„ä½ç½®å’Œæ–¹å¼çš„å…·ä½“è¿‡ç¨‹ï¼Œä½†é‡è¦çš„æ˜¯åœ¨æ¯ä¸ªæ­¥éª¤ä¸­éƒ½è¿›è¡Œäº†éªŒè¯ã€‚æ­£å¦‚åœ¨[æµ‹è¯•è¯¾ç¨‹](https://franztao.github.io/2022/10/01/Testing/#data)ä¸­æ‰€åšçš„é‚£æ ·ï¼Œå°†å†æ¬¡ä½¿ç”¨[Great Expectations](https://greatexpectations.io/)åœ¨è½¬æ¢æ•°æ®ä¹‹å‰éªŒè¯æå–å’ŒåŠ è½½çš„æ•°æ®ã€‚

é€šè¿‡ç›®å‰å­¦ä¹ çš„ Airflow æ¦‚å¿µï¼Œå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼ä½¿ç”¨transformersæ•°æ®éªŒè¯åº“æ¥éªŒè¯transformersæ•°æ®ã€‚æ— è®ºä½¿ç”¨ä»€ä¹ˆæ•°æ®éªŒè¯å·¥å…·ï¼ˆä¾‹å¦‚[Great Expectations](https://greatexpectations.io/)ã€[TFX](https://www.tensorflow.org/tfx/data_validation/get_started)ã€[AWS Deequ](https://github.com/awslabs/deequ)ç­‰ï¼‰ï¼Œéƒ½å¯ä»¥ä½¿ç”¨ BashOperatorã€PythonOperator ç­‰æ¥è¿è¡Œtransformersæµ‹è¯•ã€‚ä½†æ˜¯ï¼ŒGreat Expectations æœ‰ä¸€ä¸ª[Airflow Provider åŒ…](https://github.com/great-expectations/airflow-provider-great-expectations)ï¼Œå¯ä»¥æ›´è½»æ¾åœ°éªŒè¯transformersæ•°æ®ã€‚è¿™ä¸ªåŒ…åŒ…å«ä¸€ä¸ª[`GreatExpectationsOperator`](https://registry.astronomer.io/providers/great-expectations/modules/greatexpectationsoperator)å¯ä»¥ç”¨æ¥å°†ç‰¹å®šæ£€æŸ¥ç‚¹ä½œä¸ºä»»åŠ¡æ‰§è¡Œçš„ã€‚

```
pip install airflow-provider-great-expectations==0.1.1 great-expectations==0.15.19
great_expectations init
```

è¿™å°†åœ¨transformersæ•°æ®å·¥ç¨‹å­˜å‚¨åº“ä¸­åˆ›å»ºä»¥ä¸‹ç›®å½•ï¼š

```
tests/great_expectations/
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ expectations/
â”œâ”€â”€ plugins/
â”œâ”€â”€ uncommitted/
â”œâ”€â”€ .gitignore
â””â”€â”€ great_expectations.yml
```

#### æ•°æ®æº

ä½†é¦–å…ˆï¼Œåœ¨åˆ›å»ºæµ‹è¯•ä¹‹å‰ï¼Œéœ€è¦`datasource`åœ¨ Great Expectations ä¸­ä¸ºtransformers Google BigQuery æ•°æ®ä»“åº“å®šä¹‰ä¸€ä¸ªæ–°çš„ã€‚è¿™å°†éœ€è¦å‡ ä¸ªåŒ…å’Œå‡ºå£ï¼š

```
pip install pybigquery==0.10.2 sqlalchemy_bigquery==1.4.4
export GOOGLE_APPLICATION_CREDENTIALS=/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json  # REPLACE
```

```
great_expectations datasource new
```

```
What data would you like Great Expectations to connect to?
    1. Files on a filesystem (for processing with Pandas or Spark)
    2. Relational database (SQL) ğŸ‘ˆ
```

```
What are you processing your files with?
1. MySQL
2. Postgres
3. Redshift
4. Snowflake
5. BigQuery ğŸ‘ˆ
6. other - Do you have a working SQLAlchemy connection string?
```

è¿™å°†æ‰“å¼€ä¸€ä¸ªäº¤äº’å¼noteï¼Œå¯ä»¥åœ¨å…¶ä¸­å¡«å†™ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ï¼š

```
datasource_name = â€œdwh"
connection_string = â€œbigquery://made-with-ml-359923/mlops_courseâ€
```

#### Suite

æ¥ä¸‹æ¥ï¼Œå¯ä»¥ä¸ºtransformersæ•°æ®assertåˆ›å»º[ä¸€å¥—æœŸæœ›ï¼š](https://franztao.github.io/2022/10/01/Testing/#suites)

```
great_expectations suite new
```

```
How would you like to create your Expectation Suite?
    1. Manually, without interacting with a sample batch of data (default)
    2. Interactively, with a sample batch of data ğŸ‘ˆ
    3. Automatically, using a profiler
```

```
Select a datasource
    1. dwh ğŸ‘ˆ
```

```
Which data asset (accessible by data connector "default_inferred_data_connector_name") would you like to use?
    1. mlops_course.projects ğŸ‘ˆ
    2. mlops_course.tags
```

```
Name the new Expectation Suite [mlops.projects.warning]: projects
```

è¿™å°†æ‰“å¼€ä¸€ä¸ªäº¤äº’å¼noteï¼Œå¯ä»¥åœ¨å…¶ä¸­å®šä¹‰transformersæœŸæœ›ã€‚ä¸ºtransformersæ ‡ç­¾æ•°æ®assertåˆ›å»ºsuiteä¹Ÿé‡å¤ç›¸åŒçš„æ“ä½œã€‚

> Expectations forÂ `mlops_course.projects`
> 
> Table expectations
> 
> ```
> # data leak
> validator.expect_compound_columns_to_be_unique(column_list=["title", "description"])
> ```

> Column expectations:
> 
> ```
> # id
> validator.expect_column_values_to_be_unique(column="id")
> 
> # create_on
> validator.expect_column_values_to_not_be_null(column="created_on")
> 
> # title
> validator.expect_column_values_to_not_be_null(column="title")
> validator.expect_column_values_to_be_of_type(column="title", type_="STRING")
> 
> # description
> validator.expect_column_values_to_not_be_null(column="description")
> validator.expect_column_values_to_be_of_type(column="description", type_="STRING")
> ```

> Expectations forÂ `mlops_course.tags`
> 
> Column expectations:
> 
> ```
> # id
> validator.expect_column_values_to_be_unique(column="id")
> 
> # tag
> validator.expect_column_values_to_not_be_null(column="tag")
> validator.expect_column_values_to_be_of_type(column="tag", type_="STRING")
> ```

#### æ£€æŸ¥ç‚¹

ä¸€æ—¦æœ‰äº†ä¸€å¥—æœŸæœ›ï¼Œå°±å¯ä»¥æ£€æŸ¥[æ£€æŸ¥ç‚¹](https://franztao.github.io/2022/10/01/Testing/#checkpoints)æ¥æ‰§è¡Œè¿™äº›æœŸæœ›ï¼š

`great_expectations checkpoint new projects`

å½“ç„¶ï¼Œè¿™å°†æ‰“å¼€ä¸€ä¸ªäº¤äº’å¼noteã€‚åªéœ€ç¡®ä¿ä»¥ä¸‹ä¿¡æ¯æ­£ç¡®ï¼ˆé»˜è®¤å€¼å¯èƒ½ä¸æ­£ç¡®ï¼‰ï¼š

```
datasource_name: dwh
data_asset_name: mlops_course.projects
expectation_suite_name: projects
```

å¹¶é‡å¤ç›¸åŒçš„æ“ä½œä¸ºtransformersæ ‡ç­¾suiteåˆ›å»ºæ£€æŸ¥ç‚¹ã€‚

#### ä»»åŠ¡

å®šä¹‰æ£€æŸ¥ç‚¹åï¼Œå°±å¯ä»¥å°†å®ƒä»¬åº”ç”¨äºä»“åº“ä¸­çš„æ•°æ®assertã€‚

```
GE_ROOT_DIR = Path(BASE_DIR, "great_expectations")

@dag(...)
def dataops():
    ...
    validate_projects = GreatExpectationsOperator(
        task_id="validate_projects",
        checkpoint_name="projects",
        data_context_root_dir=GE_ROOT_DIR,
        fail_task_on_validation_failure=True,
    )
    validate_tags = GreatExpectationsOperator(
        task_id="validate_tags",
        checkpoint_name="tags",
        data_context_root_dir=GE_ROOT_DIR,
        fail_task_on_validation_failure=True,
    )

    # Define DAG
    extract_and_load_projects >> validate_projects
    extract_and_load_tags >> validate_tags
```

### è½¬æ¢

ä¸€æ—¦éªŒè¯äº†æå–å’ŒåŠ è½½çš„æ•°æ®ï¼Œå°±å‡†å¤‡å¥½[è½¬æ¢](https://franztao.github.io/2022/11/10/Data_stack/#transform)å®ƒäº†ã€‚transformers DataOps å·¥ä½œæµç¨‹å¹¶ä¸ç‰¹å®šäºä»»ä½•ç‰¹å®šçš„ä¸‹æ¸¸åº”ç”¨ç¨‹åºï¼Œå› æ­¤è½¬æ¢å¿…é¡»å…·æœ‰å…¨å±€ç›¸å…³æ€§ï¼ˆä¾‹å¦‚æ¸…ç†ç¼ºå¤±æ•°æ®ã€èšåˆç­‰ï¼‰ã€‚å°±åƒåœ¨transformers[æ•°æ®å †æ ˆè¯¾ç¨‹](https://franztao.github.io/2022/11/10/Data_stack/)ä¸­ä¸€æ ·ï¼Œå°†ä½¿ç”¨[dbt](https://www.getdbt.com/)æ¥è½¬æ¢transformersæ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ä¸€æ¬¡ï¼Œå°†ä½¿ç”¨å¼€æº[dbt-core](https://github.com/dbt-labs/dbt-core)åŒ…ä»¥ç¼–ç¨‹æ–¹å¼å®Œæˆæ‰€æœ‰å·¥ä½œã€‚

åœ¨transformersæ•°æ®å·¥ç¨‹å­˜å‚¨åº“çš„æ ¹ç›®å½•ä¸­ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆå§‹åŒ–transformers dbt ç›®å½•ï¼š

```
dbt init dbf_transforms
```

```
Which database would you like to use?
[1] bigquery ğŸ‘ˆ
```

```
Desired authentication method option:
[1] oauth
[2] service_account ğŸ‘ˆ
```

```
keyfile: /Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json  # REPLACE
project (GCP project id): made-with-ml-XXXXXX  # REPLACE
dataset: mlops_course
threads: 1
job_execution_timeout_seconds: 300
```

```
Desired location option:
[1] US  ğŸ‘ˆ  # or what you picked when defining your dataset in Airbyte DWH destination setup
[2] EU
```

#### Models

å°†åƒåœ¨ä¸Šä¸€è¯¾ä¸­ä½¿ç”¨[dbt Cloud IDE](https://franztao.github.io/2022/11/10/Data_stack/#dbt-cloud)ä¸€æ ·å‡†å¤‡transformers dbt æ¨¡å‹ã€‚

```
cd dbt_transforms
rm -rf models/example
mkdir models/labeled_projects
touch models/labeled_projects/labeled_projects.sql
touch models/labeled_projects/schema.yml
```

å¹¶å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°transformersæ¨¡å‹æ–‡ä»¶ä¸­ï¼š

```
-- models/labeled_projects/labeled_projects.sql
SELECT p.id, created_on, title, description, tag
FROM `made-with-ml-XXXXXX.mlops_course.projects` p  -- REPLACE
LEFT JOIN `made-with-ml-XXXXXX.mlops_course.tags` t  -- REPLACE
ON p.id = t.id
```

```
# models/labeled_projects/schema.yml

version: 2

models:
    - name: labeled_projects
      description: "Tags for all projects"
      columns:
          - name: id
            description: "Unique ID of the project."
            tests:
                - unique
                - not_null
          - name: title
            description: "Title of the project."
            tests:
                - not_null
          - name: description
            description: "Description of the project."
            tests:
                - not_null
          - name: tag
            description: "Labeled tag for the project."
            tests:
                - not_null
```

å¯ä»¥ä½¿ç”¨ BashOperator æ¥æ‰§è¡Œtransformers dbt å‘½ä»¤ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```
DBT_ROOT_DIR = Path(BASE_DIR, "dbt_transforms")

@dag(...)
def dataops():
    ...
    # Transform
    transform = BashOperator(task_id="transform", bash_command=f"cd {DBT_ROOT_DIR} && dbt run && dbt test")

    # Define DAG
    extract_and_load_projects >> validate_projects
    extract_and_load_tags >> validate_tags
    [validate_projects, validate_tags] >> transform
```

> ä»¥ç¼–ç¨‹æ–¹å¼ä½¿ç”¨ dbt Cloud
> 
> å½“åœ¨æœ¬åœ°å¼€å‘æ—¶ï¼Œå¯ä»¥è½»æ¾åœ°ä½¿ç”¨ Airflow çš„[dbt äº‘æä¾›å•†](https://airflow.apache.org/docs/apache-airflow-providers-dbt-cloud/)è¿æ¥åˆ°transformers dbt äº‘å¹¶ä½¿ç”¨ä¸åŒçš„æ“ä½œå‘˜æ¥å®‰æ’ä½œä¸šã€‚æ¨èç”¨äºç”Ÿäº§ï¼Œå› ä¸ºå¯ä»¥è®¾è®¡å…·æœ‰é€‚å½“ç¯å¢ƒã€èº«ä»½éªŒè¯ã€æ¨¡å¼ç­‰çš„ä½œä¸šã€‚
> 
> - å°† Airflow ä¸ dbt Cloud è¿æ¥ï¼š

> è½¬åˆ°ç®¡ç† > è¿æ¥ > +
> 
> ```
> Connection ID: dbt_cloud_default
> Connection Type: dbt Cloud
> Account ID: View in URL of https://cloud.getdbt.com/
> API Token: View in https://cloud.getdbt.com/#/profile/api/
> ```
> 
> - è½¬æ¢

> `pip install apache-airflow-providers-dbt-cloud==2.1.0`

> ```
> from airflow.providers.dbt.cloud.operators.dbt import DbtCloudRunJobOperator
> transform = DbtCloudRunJobOperator(
>     task_id="transform",
>     job_id=118680,  # Go to dbt UI > click left menu > Jobs > Transform > job_id in URL
>     wait_for_termination=True,  # wait for job to finish running
>     check_interval=10,  # check job status
>     timeout=300,  # max time for job to execute
> )
> ```

#### è¯å®

å½“ç„¶ï¼Œä¼šå¸Œæœ›ä½¿ç”¨è¿œå¤§çš„æœŸæœ›æ¥éªŒè¯è¶…å‡º dbt å†…ç½®æ–¹æ³•çš„è½¬æ¢ã€‚å°†åƒä¸Šé¢ä¸ºtransformersé¡¹ç›®å’Œæ ‡ç­¾æ•°æ®assertæ‰€åšçš„é‚£æ ·åˆ›å»ºä¸€ä¸ªsuiteå’Œæ£€æŸ¥ç‚¹ã€‚

```
great_expectations suite new  # for mlops_course.labeled_projects

```

> Expectations forÂ `mlops_course.labeled_projects`
> 
> Table expectations
> 
> ```
> # data leak
> validator.expect_compound_columns_to_be_unique(column_list=["title", "description"])
> 
> ```

> Column expectations:
> 
> ```
> # id
> validator.expect_column_values_to_be_unique(column="id")
> 
> # create_on
> validator.expect_column_values_to_not_be_null(column="created_on")
> 
> # title
> validator.expect_column_values_to_not_be_null(column="title")
> validator.expect_column_values_to_be_of_type(column="title", type_="STRING")
> 
> # description
> validator.expect_column_values_to_not_be_null(column="description")
> validator.expect_column_values_to_be_of_type(column="description", type_="STRING")
> 
> # tag
> validator.expect_column_values_to_not_be_null(column="tag")
> validator.expect_column_values_to_be_of_type(column="tag", type_="STRING")
> 
> ```



`great_expectations checkpoint new labeled_projects`

```
datasource_name: dwh
data_asset_name: mlops_course.labeled_projects
expectation_suite_name: labeled_projects

```

å°±åƒä¸ºæå–å’ŒåŠ è½½çš„æ•°æ®æ·»åŠ éªŒè¯ä»»åŠ¡ä¸€æ ·ï¼Œå¯ä»¥åœ¨ Airflow ä¸­å¯¹è½¬æ¢åçš„æ•°æ®æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼š

```
@dag(...)
def dataops():
    ...
    # Transform
    transform = BashOperator(task_id="transform", bash_command=f"cd {DBT_ROOT_DIR} && dbt run && dbt test")
    validate_transforms = GreatExpectationsOperator(
        task_id="validate_transforms",
        checkpoint_name="labeled_projects",
        data_context_root_dir=GE_ROOT_DIR,
        fail_task_on_validation_failure=True,
    )

    # Define DAG
    extract_and_load_projects >> validate_projects
    extract_and_load_tags >> validate_tags
    [validate_projects, validate_tags] >> transform >> validate_transforms

```



---

ç°åœ¨å·²ç»å®šä¹‰å¹¶æ‰§è¡Œäº†æ•´ä¸ª DataOps DAGï¼Œå®ƒå°†ä¸º[ä¸‹æ¸¸åº”ç”¨ç¨‹åº](https://franztao.github.io/2022/11/10/Data_stack/#applications)å‡†å¤‡ä»æå–åˆ°åŠ è½½å†åˆ°è½¬æ¢ï¼ˆå¹¶åœ¨æ¯ä¸ªæ­¥éª¤ä¸­è¿›è¡ŒéªŒè¯ï¼‰çš„æ•°æ®ã€‚

![æ•°æ®æ“ä½œ](https://madewithml.com/static/images/mlops/orchestration/dataops.png)

> é€šå¸¸ï¼Œä¼šä½¿ç”¨[ä¼ æ„Ÿå™¨](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/sensors/)åœ¨æ»¡è¶³æ¡ä»¶æ—¶è§¦å‘å·¥ä½œæµï¼Œæˆ–è€…é€šè¿‡ API è°ƒç”¨ç­‰ç›´æ¥ä»å¤–éƒ¨æºè§¦å‘å®ƒä»¬ã€‚å¯¹äºtransformers ML ç”¨ä¾‹ï¼Œè¿™å¯èƒ½æ˜¯å®šæœŸçš„ï¼Œæˆ–è€…åœ¨æ ‡è®°æˆ–ç›‘æ§å·¥ä½œæµæ—¶è§¦å‘é‡æ–°è®­ç»ƒç­‰.

## MLOps

å‡†å¤‡å¥½æ•°æ®åï¼Œå°±å¯ä»¥åˆ›å»ºè®¸å¤šä¾èµ–å®ƒçš„æ½œåœ¨ä¸‹æ¸¸åº”ç”¨ç¨‹åºä¹‹ä¸€ã€‚è®©å›åˆ°transformers`mlops-course`é¡¹ç›®å¹¶æŒ‰ç…§ç›¸åŒçš„ Airflow[è®¾ç½®è¯´æ˜](https://franztao.github.io/2022/11/10/Orchestration/#set-up)è¿›è¡Œæ“ä½œï¼ˆæ‚¨å¯ä»¥ä»transformersæ•°æ®å·¥ç¨‹é¡¹ç›®ä¸­åœæ­¢ Airflow ç½‘ç»œæœåŠ¡å™¨å’Œè°ƒåº¦ç¨‹åºï¼Œå› ä¸ºå°†é‡ç”¨ç«¯å£ 8000ï¼‰ã€‚

```
# Airflow webserver
source venv/bin/activate
export AIRFLOW_HOME=${PWD}/airflow
export GOOGLE_APPLICATION_CREDENTIALS=/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE
airflow webserver --port 8080
# Go to http://localhost:8080

```

```
# Airflow scheduler
source venv/bin/activate
export AIRFLOW_HOME=${PWD}/airflow
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
export GOOGLE_APPLICATION_CREDENTIALS=~/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json # REPLACE
airflow scheduler

```

`touch airflow/dags/workflows.py`

```
# airflow/dags/workflows.py
from pathlib import Path
from airflow.decorators import dag
from airflow.utils.dates import days_ago

# Default DAG args
default_args = {
    "owner": "airflow",
    "catch_up": False,
}

@dag(
    dag_id="mlops",
    description="MLOps tasks.",
    default_args=default_args,
    schedule_interval=None,
    start_date=days_ago(2),
    tags=["mlops"],
)
def mlops():
    """MLOps workflows."""
    pass

# Run DAG
ml = mlops()

```

### æ•°æ®é›†

å·²ç»`tagifai.elt_data`å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°æ¥å‡†å¤‡transformersæ•°æ®ï¼Œä½†å¦‚æœæƒ³åˆ©ç”¨æ•°æ®ä»“åº“ä¸­çš„æ•°æ®ï¼Œéœ€è¦è¿æ¥åˆ°å®ƒã€‚

`pip install google-cloud-bigquery==1.21.0`

```
# airflow/dags/workflows.py
from google.cloud import bigquery
from google.oauth2 import service_account

PROJECT_ID = "made-with-ml-XXXXX" # REPLACE
SERVICE_ACCOUNT_KEY_JSON = "/Users/goku/Downloads/made-with-ml-XXXXXX-XXXXXXXXXXXX.json"  # REPLACE

def _extract_from_dwh():
    """Extract labeled data from
    our BigQuery data warehouse and
    save it locally."""
    # Establish connection to DWH
    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_KEY_JSON)
    client = bigquery.Client(credentials=credentials, project=PROJECT_ID)

    # Query data
    query_job = client.query("""
        SELECT *
        FROM mlops_course.labeled_projects""")
    results = query_job.result()
    results.to_dataframe().to_csv(Path(config.DATA_DIR, "labeled_projects.csv"), index=False)

@dag(
    dag_id="mlops",
    description="MLOps tasks.",
    default_args=default_args,
    schedule_interval=None,
    start_date=days_ago(2),
    tags=["mlops"],
)
def mlops():
    """MLOps workflows."""
    extract_from_dwh = PythonOperator(
        task_id="extract_data",
        python_callable=_extract_from_dwh,
    )

    # Define DAG
    extract_from_dwh

```

### è¯å®

æ¥ä¸‹æ¥ï¼Œå°†ä½¿ç”¨ Great Expectations æ¥éªŒè¯transformersæ•°æ®ã€‚å°½ç®¡å·²ç»éªŒè¯äº†transformersæ•°æ®ï¼Œä½†æœ€å¥½çš„åšæ³•æ˜¯åœ¨æ•°æ®ä»ä¸€ä¸ªåœ°æ–¹ç§»äº¤åˆ°å¦ä¸€ä¸ªåœ°æ–¹æ—¶æµ‹è¯•æ•°æ®è´¨é‡ã€‚å·²ç»`labeled_projects`åœ¨[æµ‹è¯•è¯¾ç¨‹](https://franztao.github.io/2022/10/01/Testing/#checkpoints)ä¸­ä¸ºåˆ›å»ºäº†ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œå› æ­¤å°†åœ¨ MLOps DAG ä¸­åˆ©ç”¨å®ƒã€‚

`pip install airflow-provider-great-expectations==0.1.1 great-expectations==0.15.19`

```
from great_expectations_provider.operators.great_expectations import GreatExpectationsOperator
from config import config

GE_ROOT_DIR = Path(config.BASE_DIR, "tests", "great_expectations")

@dag(...)
def mlops():
    """MLOps workflows."""
    extract_from_dwh = PythonOperator(
        task_id="extract_data",
        python_callable=_extract_from_dwh,
    )
    validate = GreatExpectationsOperator(
        task_id="validate",
        checkpoint_name="labeled_projects",
        data_context_root_dir=GE_ROOT_DIR,
        fail_task_on_validation_failure=True,
    )

    # Define DAG
    extract_from_dwh >> validate

```

### train

æœ€åï¼Œå°†ä½¿ç”¨ç»è¿‡éªŒè¯çš„æ•°æ®ä¼˜åŒ–å’Œè®­ç»ƒæ¨¡å‹ã€‚

```
from airflow.operators.python_operator import PythonOperator
from config import config
from tagifai import main

@dag(...)
def mlops():
    """MLOps workflows."""
    ...
    optimize = PythonOperator(
        task_id="optimize",
        python_callable=main.optimize,
        op_kwargs={
            "args_fp": Path(config.CONFIG_DIR, "args.json"),
            "study_name": "optimization",
            "num_trials": 1,
        },
    )
    train = PythonOperator(
        task_id="train",
        python_callable=main.train_model,
        op_kwargs={
            "args_fp": Path(config.CONFIG_DIR, "args.json"),
            "experiment_name": "baselines",
            "run_name": "sgd",
        },
    )

```

---

è¿™æ ·å°±å®šä¹‰äº† MLOps å·¥ä½œæµï¼Œå®ƒä½¿ç”¨ä»transformers DataOps å·¥ä½œæµä¸­å‡†å¤‡å¥½çš„æ•°æ®ã€‚æ­¤æ—¶ï¼Œå¯ä»¥æ·»åŠ é¢å¤–çš„ç¦»çº¿/åœ¨çº¿è¯„ä¼°ã€éƒ¨ç½²ç­‰ä»»åŠ¡ï¼Œæµç¨‹åŒä¸Šã€‚

![mloops](https://madewithml.com/static/images/mlops/orchestration/mlops.png)

## æŒç»­å­¦ä¹ 

DataOps å’Œ MLOps å·¥ä½œæµè¿æ¥èµ·æ¥åˆ›å»ºä¸€ä¸ªèƒ½å¤ŸæŒç»­å­¦ä¹ çš„ ML ç³»ç»Ÿã€‚è¿™æ ·çš„ç³»ç»Ÿå°†æŒ‡å¯¼ä½•æ—¶æ›´æ–°ã€ç¡®åˆ‡æ›´æ–°ä»€ä¹ˆä»¥åŠå¦‚ä½•ï¼ˆè½»æ¾ï¼‰æ›´æ–°ã€‚

> ä½¿ç”¨è¿ç»­ï¼ˆæœ‰ä¸­æ–­çš„é‡å¤ï¼‰è¿™ä¸ªè¯è€Œä¸æ˜¯è¿ç»­çš„ï¼ˆæ²¡æœ‰ä¸­æ–­/å¹²é¢„çš„é‡å¤ï¼‰ï¼Œå› ä¸ºä¸æ˜¯è¦åˆ›å»ºä¸€ä¸ªç³»ç»Ÿï¼Œå®ƒä¼šåœ¨æ²¡æœ‰äººä¸ºå¹²é¢„çš„æƒ…å†µä¸‹è‡ªåŠ¨æ›´æ–°æ–°çš„ä¼ å…¥æ•°æ®ã€‚

### ç›‘æ§

transformersç”Ÿäº§ç³»ç»Ÿæ˜¯å®æ—¶çš„å¹¶[å—åˆ°ç›‘æ§](https://franztao.github.io/2022/10/27/monitoring/)ã€‚å½“æ„Ÿå…´è¶£çš„äº‹ä»¶å‘ç”Ÿæ—¶ï¼ˆä¾‹å¦‚[drift](https://franztao.github.io/2022/10/27/monitoring/#drift)ï¼‰ï¼Œéœ€è¦è§¦å‘å‡ ä¸ªäº‹ä»¶ä¹‹ä¸€ï¼š

- `continue`ï¼šä½¿ç”¨å½“å‰éƒ¨ç½²çš„æ¨¡å‹ï¼Œæ²¡æœ‰ä»»ä½•æ›´æ–°ã€‚ä½†æ˜¯ï¼Œå·²å‘å‡ºè­¦æŠ¥ï¼Œå› æ­¤åº”ç¨åå¯¹å…¶è¿›è¡Œåˆ†æä»¥å‡å°‘è¯¯æŠ¥è­¦æŠ¥ã€‚
- `improve`ï¼šé€šè¿‡é‡æ–°è®­ç»ƒæ¨¡å‹æ¥é¿å…æœ‰æ„ä¹‰çš„æ¼‚ç§»ï¼ˆæ•°æ®ã€ç›®æ ‡ã€æ¦‚å¿µç­‰ï¼‰å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚
- `inspect`ï¼š åšä¸€ä¸ªå†³å®šã€‚é€šå¸¸ä¼šé‡æ–°è¯„ä¼°æœŸæœ›ï¼Œé‡æ–°è¯„ä¼°æ¨¡å¼ä»¥è¿›è¡Œæ›´æ”¹ï¼Œé‡æ–°è¯„ä¼°åˆ‡ç‰‡ç­‰ã€‚
- `rollback`ï¼šç”±äºå½“å‰éƒ¨ç½²å­˜åœ¨é—®é¢˜è€Œå¯¼è‡´æ¨¡å‹çš„å…ˆå‰ç‰ˆæœ¬ã€‚é€šå¸¸å¯ä»¥ä½¿ç”¨ç¨³å¥çš„éƒ¨ç½²ç­–ç•¥ï¼ˆä¾‹å¦‚æš—é‡‘ä¸é›€ï¼‰æ¥é¿å…è¿™äº›é—®é¢˜ã€‚

### å†åŸ¹è®­

å¦‚æœéœ€è¦æ”¹è¿›æ¨¡å‹çš„ç°æœ‰ç‰ˆæœ¬ï¼Œè¿™ä¸ä»…ä»…æ˜¯åœ¨æ–°æ•°æ®é›†ä¸Šé‡æ–°è¿è¡Œæ¨¡å‹åˆ›å»ºå·¥ä½œæµç¨‹è¿™ä¸€äº‹å®ã€‚éœ€è¦ä»”ç»†ç»„åˆè®­ç»ƒæ•°æ®ï¼Œä»¥é¿å…ç¾éš¾æ€§é—å¿˜ï¼ˆåœ¨å‘ˆç°æ–°æ•°æ®æ—¶å¿˜è®°ä»¥å‰å­¦ä¹ çš„æ¨¡å¼ï¼‰ç­‰é—®é¢˜ã€‚

- `labeling`ï¼šæ–°ä¼ å…¥çš„æ•°æ®åœ¨ä½¿ç”¨å‰å¯èƒ½éœ€è¦æ­£ç¡®æ ‡è®°ï¼ˆä¸èƒ½åªä¾èµ–ä»£ç†æ ‡ç­¾ï¼‰ã€‚
- `activeÂ learning`ï¼šå¯èƒ½æ— æ³•æ˜ç¡®æ ‡è®°æ¯ä¸ªæ–°æ•°æ®ç‚¹ï¼Œå› æ­¤å¿…é¡»åˆ©ç”¨[ä¸»åŠ¨å­¦ä¹ ](https://franztao.github.io/2022/10/10/labeling/#active-learning)å·¥ä½œæµç¨‹æ¥å®Œæˆæ ‡è®°è¿‡ç¨‹ã€‚
- `QA`ï¼šè´¨é‡ä¿è¯å·¥ä½œæµç¨‹ï¼Œä»¥ç¡®ä¿æ ‡ç­¾å‡†ç¡®ï¼Œå°¤å…¶æ˜¯å¯¹äºå·²çŸ¥çš„è¯¯æŠ¥/æ¼æŠ¥å’Œå†å²ä¸Šè¡¨ç°ä¸ä½³çš„æ•°æ®ç‰‡æ®µã€‚
- `augmentation`ï¼šä½¿ç”¨ä»£è¡¨åŸå§‹æ•°æ®é›†çš„[å¢å¼ºæ•°æ®](https://franztao.github.io/2022/10/10/Data_Augmentation//)å¢åŠ transformersè®­ç»ƒé›†ã€‚
- `sampling`ï¼šä¸Šé‡‡æ ·å’Œä¸‹é‡‡æ ·ä»¥è§£å†³ä¸å¹³è¡¡çš„æ•°æ®åˆ‡ç‰‡ã€‚
- `evaluation`ï¼šåˆ›å»ºä¸€ä¸ªè¯„ä¼°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä»£è¡¨æ¨¡å‹åœ¨éƒ¨ç½²åå°†é‡åˆ°çš„æƒ…å†µã€‚

ä¸€æ—¦æœ‰äº†åˆé€‚çš„æ•°æ®é›†è¿›è¡Œå†è®­ç»ƒï¼Œå°±å¯ä»¥å¯åŠ¨å·¥ä½œæµæ¥æ›´æ–°transformersç³»ç»Ÿï¼

___

æ›´å¤šå¹²è´§ï¼Œç¬¬ä¸€æ—¶é—´æ›´æ–°åœ¨ä»¥ä¸‹å¾®ä¿¡å…¬ä¼—å·ï¼š

![](https://raw.githubusercontent.com/franztao/blog_picture/main/marktext/2023-01-10-23-44-06-image.png)

æ‚¨çš„ä¸€ç‚¹ç‚¹æ”¯æŒï¼Œæ˜¯æˆ‘åç»­æ›´å¤šçš„åˆ›é€ å’Œè´¡çŒ®

![](https://raw.githubusercontent.com/franztao/blog_picture/main/marktext/2023-01-10-23-43-17-image.png)



è½¬è½½åˆ°è¯·åŒ…æ‹¬æœ¬æ–‡åœ°å€
æ›´è¯¦ç»†çš„è½¬è½½äº‹å®œè¯·å‚è€ƒ[æ–‡ç« å¦‚ä½•è½¬è½½/å¼•ç”¨](https://franztao.github.io/2022/12/04/%E6%96%87%E7%AB%A0%E5%A6%82%E4%BD%95%E8%BD%AC%E8%BD%BD%E5%92%8C%E5%BC%95%E7%94%A8/)

æœ¬æ–‡ä¸»ä½“æºè‡ªä»¥ä¸‹é“¾æ¥ï¼š

```
@article{madewithml,
    author       = {Goku Mohandas},
    title        = { Made With ML },
    howpublished = {\url{https://madewithml.com/}},
    year         = {2022}
}
```