0x00：招聘方希望从校招招到什么样的同学
不管是眼下的寒冬还是前些年的盛世，大厂的对校招同学的期望从未变过：我们始终想招具有快速成长潜力的同学。

0x01：八股文和代码：耗时最多，其实最不重要
基本上我觉得能比较顺畅的过easy题，有middle题目的思路，有点计算复杂度的概念，能讲出问题的分析思路，编程语言层面上的理解到位，相关工具能够正确使用
注意是“证明我能快速学会”，而不是“号称我能快速学会”

0x02：关于paper：重要但是也没你想的那么重要
因为如果你能做出世界级的创新工作，你应该不会来找工作。如果你像其他同学一样投简历找工作，大概率你的paper成色也就那样

0x03：人人都爱自驱
一线leaders有一次培训讨论一个问题：你最希望下属具有的素质是什么？排名第一就是自驱，这几乎是所有人的共识。

0x04：聪明vs勤奋
但是很遗憾，聪明，而不是勤奋，是大厂面试中你最需要展现的特质。

洞察力最明显的一种体现就是建模和迁移能力，如果针对面试官的问题，你能回答出这两句话：
这个问题本质上是一个XXX问题
这个XXX问题与YYY问题的思路是相通的，我们可以基于YYY问题上的研究成果设计XXX问题的解决方案如下

0x05：沟通：一直都很重要
沟通。同样是越靠后的面试越重要。讲话啰嗦，词不达意，语言理解不畅，任何一个让面试官觉得“沟通不舒服”的点都可能直接导致你拿不到offer。

0x06：性格：不是玄学
面试是很看匹配度的，所谓匹配度，一是你的自身素质和背景与当前的岗位需求是否匹配，二是你的性格特质与团队是否匹配。

0x08：请回答下面的问题或练习
方法论是虚的，你需要有一些手段来诊断自己的准备情况，不妨请一位已经工作的有经验者作为你的模拟面试官，尝试回答下面的问题

如果说你是一个聪明人，你认为你聪明在什么地方？理由是什么？

你的实习/项目/paper中，难点在哪里，为什么困难？你是如何解决的？

你迄今为止觉得自己最牛逼，最得意的一件事是什么？

你觉得工作的意义是什么，你期望中理想的工作是什么样的？请你的面试官从工作角度（而不是朋友角度）审视你的回答。

请你的模拟面试官针对你的项目进行挑战，要求面试官能够根据你的陈述，给出一个关于你项目的不同的解决方案，你来进行defense。如果面试官的方案不行，为什么不行。如果可行，你为什么没那么干：你的模拟面试官是否充分理解了你项目的目标、价值、方案逻辑？

你的defense是否有理有据有节，沟通过程中你的模拟面试官是否觉得舒服？

寻找一些开放问题，在模拟面试中限定时间回答并复盘。


四、自然语言处理基础
28.编写正则表达式，要求提取出中文中的AABB，ABAB，ABAC四字组合。
例如：给定以下文本：这是一个不真不假的故事，你一五一十的跟我说为什么会出现这样的事情，让我们几个人开心开心，这样以来，我们开心了，我们才能热热闹闹地办好这件事，然后让你开开心心的回去。 输出：不真不假、开心开心、热热闹闹、开开心心

29.TFIDF的计算公式及主要思想

30.编写正向/反向最大匹配分词算法

31.介绍一下统计语言模型

32.one-hot表示法的原理和优缺点

33.介绍textrank/pagerank提取关键词的原理

34.编写一个统计词频的程序

35.NLP有哪些常规的任务

36.什么是朴素贝叶斯算法？何时可以在NLP中使用该算法？

37.LSTM是如何实现长短期记忆功能的？

38.实现输入纠错的方法

39.依存句法分析、句子成分分析以及amr分析有什么区别和联系

40.假设语料库词总数为13748，并给定以下语料统计数据：

要求：
1）运用bi-gram计算句子‘I want to eat Chinese food’的生成概率
2）如果语料不足，通常会导致很多ngram对在语料库中不存在，这样会造成什么问题，通常要采取什么措施？

41.词向量有哪些获取方法？简述你使用过的词向量训练模型原理，如可以，画出示意图。

42.交叉熵与信息熵，相对熵（KL散度），具体如何计算？

43.交叉验证是什么？为什么使用交叉验证？

44.文本情感分析中，需要依赖外部情感字典，对情感词扩展的方法有哪些？请阐述说明。

45.深度学习中，特征选择方法有哪些？

46.BERT的具体网络结构，以及训练过程，BERT中的Embedding都包括哪几部分Bert 采用哪种Normalization结构，LayerNorm和BatchNorm区别，LayerNorm结构有参数吗，参数的作用？

47.如何优化BERT性能？BERT在工业落地时候，如何保证QPS达标？

48.如何理解Multi-head attention，Bert源码中多头的维度如何变化的，Bert是双向模型，双向体现在哪？Bert的位置编码和transformers的位置编码一样么?

49.如何解决样本不平衡的问题？都见过什么评价指标，precsion,accuray,f1-score,recall,auc的区别与含义。

50.遇到过什么数据标注问题？对于漏标、错标等数据标注问题通常都有哪些解决方案。

下面分别从实体识别与实体关系抽取、词向量CNN/RNN/LSTM相关、BERT相关的三个部分，就高频的一些问题【偏原理性】进行列举。

五、实体识别与实体关系抽取
51、隐马尔可夫模型和CRF条件随机场的区别？

52、CRF模型的优化目标是什么，怎么训练的？

53、CRF的预测方法，维特比算法的过程？

54、BILSTM+CRF的训练目标？状态转移矩阵的维度是多少？其中CRF的作用？

55、有哪些命名实体识别算法？具体的优缺点？

56、如何解决实体识别中的实体重叠、实体不连续、实体名称过长问题？

57、如何解决实体识别标注数据中的漏标、错标问题？

58、使用MRC进行实体识别中如何解决长文本问题？

59、实体关系抽取的常用方法有哪些，pipeline抽取和联合抽取的区别是什么？

60、序列标注问题中的实体标注方法有哪些，有哪些异同点？

61、实体链接的过程中，是如何获取候选实体的？是如何进行实体对齐的？

62、事件抽取的一般流程是什么？如何进行事件检测？

63、事件论元抽取的常用方法有哪些？

64、简述精确率、准确率、召回率、F1值、AUC的含义以及计算方式

65、简述指针网络的模型结构？

六、词向量/CNN/RNN相关
66、Word2vec的cbow、skipgram模型的区别？word2vec的CBOW与SkipGram模型及两种训练方式(负采样\层级softmax)， 两种训练方式的区别和应用场景？

67、Word2vec中的词向量是怎么产生的？有哪些参数？

68、TextCNN的模型构成，其物理意义是什么？GRU与LSTM的区别？

69、LSTM是如何实现长短期记忆功能的，说下LSTM的内部结构？

70、LSTM中各模块分别使用什么激活函数，可以使用别的激活函数吗？

71、RNN为什么会出现梯度消失或梯度爆炸？有哪些改进方案？

72、处理文本数据时，RNN比CNN有什么特点？

73、有哪些生成词向量的方法？

74、简述fastext词向量的工作原理，其是如何解决未登录词的？

75、激活函数有什么用？常见的激活函数的区别是什么？

七、BERT相关
76、简述一下2gram语言模型的思想，BERT的模型结构，输入输出分别是什么？

77、BERT的MASK方式的优缺点，在BERT中，token分3种情况mask，分别的作用是什么？

78、BERT深度双向的特点，双向体现在哪儿，深度体现在哪儿，并行计算体现在哪儿？

79、Transformer在哪里做了权重共享，为什么可以做权重共享？

80、BERT中Transformer中的Q、K、V存在的意义，Attention结构有什么优点？

81、BERT中Transformer中Self-attention后为什么要加前馈网络？多个头的作用？如何理解Multi-head attention？Bert源码中多头的维度如何变化的？

82、BERT中self-attention中相乘操作代表着的是什么？为什么要除以缩放因子？

83、BERT是如何分词的，对于输入的一句话，其得到的字符序列、segment序列以及mask序列是怎样的？

84、position embeddig和Residual Connection的作用是什么，BERT中位置编码的计算方式？

85、加入LayerNorm层有什么好处，layer normalization与batch normalization的区别？

86、加入NextSentence Prediction任务的目的是什么，这个任务有哪些缺点，如何改进？

87、Mask-LM的样本中，选中的词在10%的概率不做Mask保持真实的词的原因是什么？选中的词在10%的概率下不做mask，而是被随机替换成为一个其他词的目的是什么？

88、为什么即便数量很小，基于BERT做微调也能取得很好的泛化效果？

89、BERT的三个Embedding直接相加会对语义有影响吗，为什么可以相加？

90、为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？

91、为什么BERT在第一句前会加一个[CLS]标志?

92、使用BERT预训练模型为什么最多只能输入512个词，最多只能两个句子合成一句？

93、BERT中的词表可变么？有哪些特殊字符，分别代表什么含义？[unused]代表什么？

94、如何让BERT支持中文分词？

95、BERT、Transformer、GPT有什么区别？

96、BERT的embedding 向量如何得来的？

97、BERT的两个预训练任务对应的损失函数是什么？

98、BERT的加速方法有哪些？albert与BERT的区别是什么？

99、简述word2vec、Elmo、BERT的区别？

100、针对句子语义相似度/多标签分类/机器翻译翻译/文本生成的任务，利用BERT结构怎么做 fine-tuning？